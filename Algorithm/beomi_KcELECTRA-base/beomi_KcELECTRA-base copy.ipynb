{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# beomi/KcELECTRA-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 1835692544820087483\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 2254123828\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 9169485238189335735\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1650 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "라이브러리 설치가 완료되었습니다.\n",
      "absl-py==2.0.0\n",
      "accelerate==0.20.1\n",
      "annotated-types==0.6.0\n",
      "asttokens==2.4.1\n",
      "astunparse==1.6.3\n",
      "backcall==0.2.0\n",
      "blis==0.7.11\n",
      "cachetools==5.3.2\n",
      "catalogue==2.0.10\n",
      "certifi==2023.7.22\n",
      "chardet==5.2.0\n",
      "charset-normalizer==3.3.1\n",
      "click==8.1.7\n",
      "cloudpathlib==0.16.0\n",
      "colorama==0.4.6\n",
      "comm==0.1.4\n",
      "confection==0.1.3\n",
      "contourpy==1.1.1\n",
      "cycler==0.12.1\n",
      "cymem==2.0.8\n",
      "dataclasses==0.6\n",
      "debugpy==1.8.0\n",
      "decorator==5.1.1\n",
      "emoji==1.2.0\n",
      "en-core-web-sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.0/en_core_web_sm-3.7.0-py3-none-any.whl#sha256=6215d71a3212690e9aec49408a27e3fe6ad7cd6c715476e93d70dc784041e93e\n",
      "et-xmlfile==1.1.0\n",
      "exceptiongroup==1.1.3\n",
      "executing==2.0.0\n",
      "filelock==3.12.4\n",
      "flatbuffers==1.12\n",
      "fonttools==4.43.1\n",
      "fsspec==2023.10.0\n",
      "gast==0.4.0\n",
      "google-auth==2.23.3\n",
      "google-auth-oauthlib==0.4.6\n",
      "google-pasta==0.2.0\n",
      "grpcio==1.59.0\n",
      "h5py==3.1.0\n",
      "huggingface-hub==0.17.3\n",
      "idna==3.4\n",
      "importlib-metadata==6.8.0\n",
      "importlib-resources==6.1.0\n",
      "iniconfig==2.0.0\n",
      "install==1.3.5\n",
      "ipykernel==6.26.0\n",
      "ipython==8.12.3\n",
      "jedi==0.19.1\n",
      "Jinja2==3.1.2\n",
      "joblib==1.3.2\n",
      "JPype1==1.4.1\n",
      "jupyter_client==8.5.0\n",
      "jupyter_core==5.4.0\n",
      "keras==2.9.0\n",
      "Keras-Preprocessing==1.1.2\n",
      "kiwisolver==1.4.5\n",
      "konlpy==0.6.0\n",
      "Korpora==0.2.0\n",
      "kss==4.5.4\n",
      "langcodes==3.3.0\n",
      "libclang==16.0.6\n",
      "lxml==4.9.3\n",
      "Markdown==3.5\n",
      "MarkupSafe==2.1.3\n",
      "matplotlib==3.7.3\n",
      "matplotlib-inline==0.1.6\n",
      "mpmath==1.3.0\n",
      "murmurhash==1.0.10\n",
      "nest-asyncio==1.5.8\n",
      "networkx==3.1\n",
      "nltk==3.8.1\n",
      "numpy==1.24.4\n",
      "oauthlib==3.2.2\n",
      "openpyxl==3.1.2\n",
      "opt-einsum==3.3.0\n",
      "packaging==23.2\n",
      "pandas==2.0.3\n",
      "parso==0.8.3\n",
      "pecab==1.0.8\n",
      "pickleshare==0.7.5\n",
      "Pillow==10.1.0\n",
      "platformdirs==3.11.0\n",
      "pluggy==1.3.0\n",
      "preshed==3.0.9\n",
      "prompt-toolkit==3.0.39\n",
      "protobuf==3.19.6\n",
      "psutil==5.9.6\n",
      "pure-eval==0.2.2\n",
      "pyarrow==14.0.0\n",
      "pyasn1==0.5.0\n",
      "pyasn1-modules==0.3.0\n",
      "pydantic==2.4.2\n",
      "pydantic_core==2.10.1\n",
      "Pygments==2.16.1\n",
      "pykospacing @ git+https://github.com/haven-jeon/PyKoSpacing.git@d1e2f93759b1bcc74b6aaa345a62c08701e5546e\n",
      "pyparsing==3.1.1\n",
      "pytest==7.4.3\n",
      "python-dateutil==2.8.2\n",
      "pytz==2023.3.post1\n",
      "pywin32==306\n",
      "PyYAML==6.0.1\n",
      "pyzmq==25.1.1\n",
      "regex==2023.10.3\n",
      "requests==2.31.0\n",
      "requests-oauthlib==1.3.1\n",
      "rhinoMorph==4.0.1.12\n",
      "rsa==4.9\n",
      "safetensors==0.4.0\n",
      "scikit-learn==1.3.2\n",
      "scipy==1.10.1\n",
      "six==1.16.0\n",
      "smart-open==6.4.0\n",
      "soynlp==0.0.493\n",
      "spacy==3.7.2\n",
      "spacy-legacy==3.0.12\n",
      "spacy-loggers==1.0.5\n",
      "srsly==2.4.8\n",
      "stack-data==0.6.3\n",
      "sympy==1.12\n",
      "tensorboard==2.9.1\n",
      "tensorboard-data-server==0.6.1\n",
      "tensorboard-plugin-wit==1.8.1\n",
      "tensorflow==2.9.3\n",
      "tensorflow-estimator==2.9.0\n",
      "tensorflow-gpu==2.10.0\n",
      "tensorflow-io-gcs-filesystem==0.31.0\n",
      "termcolor==2.3.0\n",
      "thinc==8.2.1\n",
      "threadpoolctl==3.2.0\n",
      "tokenizers==0.13.3\n",
      "tomli==2.0.1\n",
      "torch==2.1.0\n",
      "tornado==6.3.3\n",
      "tqdm==4.66.1\n",
      "traitlets==5.12.0\n",
      "transformers==4.30.2\n",
      "typer==0.9.0\n",
      "typing_extensions==4.8.0\n",
      "tzdata==2023.3\n",
      "urllib3==2.0.7\n",
      "wasabi==1.1.2\n",
      "wcwidth==0.2.8\n",
      "weasel==0.3.3\n",
      "Werkzeug==3.0.1\n",
      "wordcloud==1.9.2\n",
      "wrapt==1.15.0\n",
      "xlrd==2.0.1\n",
      "zipp==3.17.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -eras (c:\\users\\gjaischool\\.conda\\envs\\nv38\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%run KcELECTRA-base.version.py\n",
    "!pip freeze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "devcie: cpu\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# deivce 선택\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"devcie:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>좌배 까는건 ㅇㅂ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>집에 롱 패딩만 세 개다. 10년 더 입어야지 ㅋㅋ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>개소리야 니가 빨갱이를 옹호하고 드루킹을 ㅇㅇ짓이라고 말못해서 삐진거야 빨갱아</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>세탁이라고 봐도 된다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>애새끼가 초딩도 아니고 ㅋㅋㅋㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Sentence  label\n",
       "0                                    좌배 까는건 ㅇㅂ      1\n",
       "1                 집에 롱 패딩만 세 개다. 10년 더 입어야지 ㅋㅋ      0\n",
       "2  개소리야 니가 빨갱이를 옹호하고 드루킹을 ㅇㅇ짓이라고 말못해서 삐진거야 빨갱아      1\n",
       "3                                  세탁이라고 봐도 된다      0\n",
       "4                            애새끼가 초딩도 아니고 ㅋㅋㅋㅋ      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 엑셀 파일에서 데이터프레임 읽기\n",
    "df = pd.read_excel(r\"C:\\Users\\GJAISCHOOL\\Desktop\\X_filter\\Algorithm\\dataset\\asdf.xlsx\")\n",
    "\n",
    "# 'Sentence' 칼럼의 값을 문자열로 변환하여 리스트로 저장\n",
    "str_data = df['Sentence'].astype(str).tolist()\n",
    "\n",
    "# 'Sentence' 칼럼의 값을 하나의 문자열로 결합\n",
    "all_sentences = ' '.join(str_data)\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_idx = df[df.label.isnull()].index                             # 해당 index에 null 값 확인\n",
    "df.loc[null_idx, \"Sentence\"]                                       # null 값이 존재한 인덱스의 content 값 불러오기\n",
    "\n",
    "# lable은 content의 가장 끝 문장열로 설정\n",
    "df.loc[null_idx, \"label\"] = df.loc[null_idx, \"Sentence\"].apply(lambda x: x[-1])\n",
    "\n",
    "# content는 \"\\t\" 앞부분까지의 문자열로 설정\n",
    "df.loc[null_idx, \"Sentence\"] = df.loc[null_idx, \"Sentence\"].apply(lambda x: x[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = df.sample(frac=0.8, random_state=42)                 # train(80%), test(20%) 셋 구분 \n",
    "test_data = df.drop(train_data.index)                             # 랜덤으로 샘플링(랜덤으로 숫자 배치)\n",
    "\n",
    "# 데이터셋 갯수 확인\n",
    "print('중복 제거 전 학습 데이터셋 : {}'.format(len(train_data)))\n",
    "print('중복 제거 전 테스트 데이터셋 : {}'.format(len(test_data)))\n",
    "\n",
    "# 중복 데이터 제거\n",
    "train_data.drop_duplicates(subset=[\"Sentence\"], inplace=True)\n",
    "test_data.drop_duplicates(subset=[\"Sentence\"], inplace=True)\n",
    "\n",
    "# 데이터셋 갯수 확인\n",
    "print('중복 제거 후 학습 데이터셋 : {}'.format(len(train_data)))\n",
    "print('중복 제거 후 테스트 데이터셋 : {}'.format(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ElectraTokenizer, ElectraForSequenceClassification, TFElectraForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "\n",
    "# KcELECTRA 모델 및 토크나이저 로드\n",
    "model_name = \"beomi/KcELECTRA-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>result</th>\n",
       "      <th>result_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>전에 제주도 여행 갔는데 폭포 앞에 사진 잘나오는 포토존에서 사진 찍는거 기다렸는데...</td>\n",
       "      <td>1</td>\n",
       "      <td>[전, 제주도, 여행, 가다, 폭포, 앞, 사진, 잘나다, 포토, 존, 사진, 찍다...</td>\n",
       "      <td>전 제주도 여행 가다 폭포 앞 사진 잘나다 포토 존 사진 찍다 기다리다 앞 짱 게이...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>진짜 몇시간씩 할 정도면 히로뽕맞고 하는거일수도있다경교대있을때 뽕쟁이 재소자 의무실...</td>\n",
       "      <td>1</td>\n",
       "      <td>[진짜, 시간, 하다, 정도, 히로뽕, 맞다, 있다, 경교대, 때, 뽕, 재소자, ...</td>\n",
       "      <td>진짜 시간 하다 정도 히로뽕 맞다 있다 경교대 때 뽕 재소자 의무실 가다 계호 때 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>대가리에 필터없는 연봉 30억 강사vs대가리가 없는 용접공</td>\n",
       "      <td>1</td>\n",
       "      <td>[대가리, 필터, 없다, 연봉, 강사, 대가, 없다, 용접공]</td>\n",
       "      <td>대가리 필터 없다 연봉 강사 대가 없다 용접공</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>휴게소, 마트 등의 여성전용주차장 보고 분노를 해야지</td>\n",
       "      <td>0</td>\n",
       "      <td>[휴게소, 마트, 여성, 전용, 주차장, 보다, 분노, 하다]</td>\n",
       "      <td>휴게소 마트 여성 전용 주차장 보다 분노 하다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>무게치는거에 인생버린새끼들</td>\n",
       "      <td>1</td>\n",
       "      <td>[무게, 치, 인생, 버리다, 새끼]</td>\n",
       "      <td>무게 치 인생 버리다 새끼</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>그래서 씨발롬아 PC가 나쁘다는 거야 좋다는 거야?</td>\n",
       "      <td>1</td>\n",
       "      <td>[그래서, 씨발, 롬, PC, 나쁘다, 좋다]</td>\n",
       "      <td>그래서 씨발 롬 PC 나쁘다 좋다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>아 그런거야? ㅋㅋㅋ</td>\n",
       "      <td>0</td>\n",
       "      <td>[아, 그렇다, ㅋㅋ, ㅋ]</td>\n",
       "      <td>아 그렇다 ㅋㅋ ㅋ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>박근혜 안빠는데 보수통합 3원칙 인정함</td>\n",
       "      <td>1</td>\n",
       "      <td>[박근혜, 안, 빠, 보수, 통합, 원칙, 인정]</td>\n",
       "      <td>박근혜 안 빠 보수 통합 원칙 인정</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>수술도 남자 의사가 잘한다</td>\n",
       "      <td>0</td>\n",
       "      <td>[수술, 남자, 의사, 잘, 하다]</td>\n",
       "      <td>수술 남자 의사 잘 하다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>만 나이 공식화도</td>\n",
       "      <td>0</td>\n",
       "      <td>[만, 나이, 공식화]</td>\n",
       "      <td>만 나이 공식화</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Sentence  label  \\\n",
       "40  전에 제주도 여행 갔는데 폭포 앞에 사진 잘나오는 포토존에서 사진 찍는거 기다렸는데...      1   \n",
       "67  진짜 몇시간씩 할 정도면 히로뽕맞고 하는거일수도있다경교대있을때 뽕쟁이 재소자 의무실...      1   \n",
       "15                   대가리에 필터없는 연봉 30억 강사vs대가리가 없는 용접공      1   \n",
       "68                      휴게소, 마트 등의 여성전용주차장 보고 분노를 해야지      0   \n",
       "88                                     무게치는거에 인생버린새끼들      1   \n",
       "..                                                ...    ...   \n",
       "60                       그래서 씨발롬아 PC가 나쁘다는 거야 좋다는 거야?      1   \n",
       "71                                        아 그런거야? ㅋㅋㅋ      0   \n",
       "14                              박근혜 안빠는데 보수통합 3원칙 인정함      1   \n",
       "92                                     수술도 남자 의사가 잘한다      0   \n",
       "51                                          만 나이 공식화도      0   \n",
       "\n",
       "                                               result  \\\n",
       "40  [전, 제주도, 여행, 가다, 폭포, 앞, 사진, 잘나다, 포토, 존, 사진, 찍다...   \n",
       "67  [진짜, 시간, 하다, 정도, 히로뽕, 맞다, 있다, 경교대, 때, 뽕, 재소자, ...   \n",
       "15                 [대가리, 필터, 없다, 연봉, 강사, 대가, 없다, 용접공]   \n",
       "68                 [휴게소, 마트, 여성, 전용, 주차장, 보다, 분노, 하다]   \n",
       "88                               [무게, 치, 인생, 버리다, 새끼]   \n",
       "..                                                ...   \n",
       "60                          [그래서, 씨발, 롬, PC, 나쁘다, 좋다]   \n",
       "71                                    [아, 그렇다, ㅋㅋ, ㅋ]   \n",
       "14                        [박근혜, 안, 빠, 보수, 통합, 원칙, 인정]   \n",
       "92                                [수술, 남자, 의사, 잘, 하다]   \n",
       "51                                       [만, 나이, 공식화]   \n",
       "\n",
       "                                           result_str  \n",
       "40  전 제주도 여행 가다 폭포 앞 사진 잘나다 포토 존 사진 찍다 기다리다 앞 짱 게이...  \n",
       "67  진짜 시간 하다 정도 히로뽕 맞다 있다 경교대 때 뽕 재소자 의무실 가다 계호 때 ...  \n",
       "15                          대가리 필터 없다 연봉 강사 대가 없다 용접공  \n",
       "68                          휴게소 마트 여성 전용 주차장 보다 분노 하다  \n",
       "88                                     무게 치 인생 버리다 새끼  \n",
       "..                                                ...  \n",
       "60                                 그래서 씨발 롬 PC 나쁘다 좋다  \n",
       "71                                         아 그렇다 ㅋㅋ ㅋ  \n",
       "14                                박근혜 안 빠 보수 통합 원칙 인정  \n",
       "92                                      수술 남자 의사 잘 하다  \n",
       "51                                           만 나이 공식화  \n",
       "\n",
       "[76 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_sentences = tokenizer(\n",
    "    list(train_data[\"Sentence\"]),\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=128,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    add_special_tokens=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test_sentences = tokenizer(\n",
    "    list(test_data[\"Sentence\"]),\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=128,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    add_special_tokens=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding(num_tokens=59, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
      "['ĠìłĦ', 'Ġìłľì£¼ëıĦ', 'ĠìĹ¬íĸī', 'Ġê°Ģ', 'ëĭ¤', 'ĠíıŃ', 'íı¬', 'Ġìķŀ', 'ĠìĤ¬ì§Ħ', 'Ġìŀĺ', 'ëĤĺëĭ¤', 'Ġíı¬íĨł', 'Ġì¡´', 'ĠìĤ¬ì§Ħ', 'Ġì°į', 'ëĭ¤', 'Ġê¸°ëĭ¤', 'ë¦¬ëĭ¤', 'Ġìķŀ', 'Ġì§±', 'Ġê²ĮìĿ´', 'Ġì§ľì¦Ŀ', 'ĠëĤ¨', 'ëĭ¤', 'Ġë¨¼ìłĢ', 'Ġê°ģìŀĲ', 'Ġíķľìŀ¥', 'Ġì°į', 'ëĭ¤', 'ĠìĿĮ', 'Ġëĭ¤ìĿĮ', 'Ġë²Ī', 'ê°Ī', 'ëĭ¤', 'Ġì§Ŀ', 'Ġì§ĵ', 'ëĭ¤', 'Ġì°į', 'ëĭ¤', 'Ġìłķë§Ĳ', 'ĠëģĿëĤĺ', 'ëĭ¤', 'Ġì°į', 'ëĭ¤', 'Ġê·¸', 'Ġëĭ¤', 'Ġëª¨', 'ìĿ´ëĭ¤', 'Ġíı¬', 'ì¦Ī', 'Ġë°Ķê¾¸', 'ëĭ¤', 'ĠëĺĲ', 'ĠìĤ¬ì§Ħ', 'Ġì°į', 'ëĭ¤', 'ĠìĿĮ', 'ĠìĿ´ì§Ģ', 'Ġíķĺëĭ¤']\n",
      "[502, 7050, 3994, 389, 237, 1283, 1520, 1124, 2519, 500, 9178, 26272, 1790, 2519, 1379, 237, 2846, 6527, 1124, 1762, 11350, 3706, 631, 237, 1858, 8840, 20856, 1379, 237, 1677, 1836, 2488, 1340, 237, 4745, 1381, 237, 1379, 237, 939, 5155, 237, 1379, 237, 434, 374, 463, 489, 1526, 2446, 3956, 237, 754, 2519, 1379, 237, 1677, 4798, 3088]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_sentences[0])        # 0번째 문장에 해당하는 객체 \n",
    "print(tokenized_train_sentences[0].tokens) # 0번째 문장에 토큰의 목록\n",
    "print(tokenized_train_sentences[0].ids)    # 0번째 문장에 대한 고유 ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CurseDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encoding, labels):\n",
    "        self.encodings = encoding\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()} # self.encodings 딕셔너리 내의 값 중에 val를 torch.tensor로 변환해 하여\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])                             # key: toch.tensor(val[ix])라는 item 딕셔너리 형성\n",
    "        return item                                                                 # 새로운 labels 키 값에 value torch.tensor(self.labels[idx]) 쌍을 생성\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)                                                     # self.labels 길이 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set, test_set에 대한 데이터셋을 각각 생성\n",
    "\n",
    "train_label = train_data[\"label\"].values\n",
    "test_label = test_data[\"label\"].values\n",
    "\n",
    "train_dataset = CurseDataset(tokenized_train_sentences, train_label)\n",
    "test_dataset = CurseDataset(tokenized_test_sentences, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at beomi/KcELECTRA-base were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at beomi/KcELECTRA-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ElectraForSequenceClassification(\n",
       "  (electra): ElectraModel(\n",
       "    (embeddings): ElectraEmbeddings(\n",
       "      (word_embeddings): Embedding(30000, 768, padding_idx=3)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): ElectraEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): ElectraClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)   # 사전 학습된 모델 찾아오기\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - 2 학습 파라미터 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = './results',           # 학습결과 저장경로\n",
    "    num_train_epochs=10,                # 학습 epoch 설정\n",
    "    per_device_train_batch_size = 8,    # train batch_size 설정\n",
    "    per_device_eval_batch_size = 64,    # test batch_size 설정\n",
    "    logging_dir = './logs',             # 학습 log 저장경로\n",
    "    logging_steps=500,                  # 학습 log 기록 단위\n",
    "    save_total_limit = 2,               # 학습결과 저장 최대갯수\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "# 학습과정에서 사용할 평가지표를 위한 함수 설정\n",
    "def compute_metrics(pred):\n",
    "  labels = pred.label_ids\n",
    "  preds = pred.predictions.argmax(-1)\n",
    "  # 정밀도, 재현율, f1 구하기 \n",
    "  precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "  # 정확도 구하기\n",
    "  acc = accuracy_score(labels, preds)\n",
    "  return{\n",
    "      'accuracy': acc,\n",
    "      'f1': f1,\n",
    "      'precision': precision,\n",
    "      'recall': recall\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer 모듈을 사용해 모델의 학습을 컨트롤하은 trainer를 생성\n",
    "trainer = Trainer(\n",
    "    model = model,                       # 학습하고자하는 Transformers model\n",
    "    args=training_args,                  # 위에서 정의한 Trainging Arguments\n",
    "    train_dataset=train_dataset,         # 학습 데이터셋\n",
    "    eval_dataset=test_dataset,           # 평가 데이터셋\n",
    "    compute_metrics=compute_metrics,     # 평가지표\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\GJAISCHOOL\\.conda\\envs\\nv38\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]C:\\Users\\GJAISCHOOL\\AppData\\Local\\Temp\\ipykernel_12548\\1167134061.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()} # self.encodings 딕셔너리 내의 값 중에 val를 torch.tensor로 변환해 하여\n",
      "100%|██████████| 100/100 [03:11<00:00,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3424, 'learning_rate': 0.0, 'epoch': 10.0}\n",
      "{'train_runtime': 191.4621, 'train_samples_per_second': 3.969, 'train_steps_per_second': 0.522, 'train_loss': 0.3423833465576172, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=0.3423833465576172, metrics={'train_runtime': 191.4621, 'train_samples_per_second': 3.969, 'train_steps_per_second': 0.522, 'train_loss': 0.3423833465576172, 'epoch': 10.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GJAISCHOOL\\AppData\\Local\\Temp\\ipykernel_12548\\1167134061.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()} # self.encodings 딕셔너리 내의 값 중에 val를 torch.tensor로 변환해 하여\n",
      "100%|██████████| 1/1 [00:00<00:00, 82.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'custom__loss': 0.45376983284950256, 'custom__accuracy': 0.8, 'custom__f1': 0.7999999999999999, 'custom__precision': 0.7272727272727273, 'custom__recall': 0.8888888888888888, 'custom__runtime': 0.4949, 'custom__samples_per_second': 40.415, 'custom__steps_per_second': 2.021, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 평가를 수행하고 custom_ 접두사가 붙은 평가 지표를 출력\n",
    "results = trainer.evaluate(eval_dataset=test_dataset, metric_key_prefix='custom_')\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, tensor([[2295,  640, 3898, 5170,   33]]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0: curse, 1: non_curse\n",
    "def sentence_predict(sent):\n",
    "    # 평가모드로 변경\n",
    "    model.eval()\n",
    "\n",
    "    # 입력된 문장 토크나이징\n",
    "    tokenized_sent = tokenizer(\n",
    "        sent,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "    # 모델이 위치한 GPU로 이동\n",
    "    # tokenized_sent.to(deivce)\n",
    "\n",
    "    # 예측\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=tokenized_sent[\"input_ids\"],\n",
    "            attention_mask=tokenized_sent[\"attention_mask\"],\n",
    "            token_type_ids=tokenized_sent[\"token_type_ids\"]\n",
    "        )\n",
    "\n",
    "    # 결과 return\n",
    "    logits = outputs[0]\n",
    "    logits = logits.detach().cpu()\n",
    "    result = logits.argmax(-1)\n",
    "    sentence = True\n",
    "    input_sentence = tokenized_sent[\"input_ids\"]\n",
    "    if result == 0:\n",
    "        sentence = True\n",
    "\n",
    "    elif result == 1:\n",
    "        sentence = False\n",
    "    return sentence, input_sentence\n",
    "\n",
    "\n",
    "sentence_predict(\"씨발이게 맞아?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입\n"
     ]
    }
   ],
   "source": [
    "def found_word(input_sentence, found_bad_word):\n",
    "    result = input_sentence\n",
    "    if found_bad_word:\n",
    "        result = badword_find(result)\n",
    "    return result\n",
    "\n",
    "def badword_find(input_sentence):\n",
    "    result = input_sentence\n",
    "    badword_df = pd.read_excel(r'C:\\Users\\user\\Desktop\\X-filter\\word_list.xlsx')\n",
    "    \n",
    "    found_bad_word = False  # 입력 문장에 단어가 발견되었는지를 나타내는 플래그\n",
    "    for idx, row in badword_df.iterrows():\n",
    "        if row[\"WORD\"] in input_sentence:\n",
    "            # 'WORD'가 입력 문장에 포함된 경우\n",
    "            new_word = row[\"대체어\"]\n",
    "            if not pd.isnull(new_word):\n",
    "                result = result.replace(row[\"WORD\"], new_word)\n",
    "                found_bad_word = True\n",
    "            else:\n",
    "                result = result.replace(row[\"WORD\"], \"*\" * len(row[\"WORD\"]))\n",
    "                found_bad_word = True\n",
    "    \n",
    "    if not found_bad_word:\n",
    "        # result = \"@\" * len(input_sentence)\n",
    "        result = \"혐오 표현입니다.\"\n",
    "    return result\n",
    "\n",
    "# 테스트\n",
    "input_sentence = \"엄마가 죽었으면 좋겠어\"\n",
    "speak_result = badword_find(input_sentence)\n",
    "print(speak_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'입'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "badword_find(input_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speak(sent):\n",
    "    sentence = sentence_predict(sent)\n",
    "    if sentence[0] == False:\n",
    "        return sent\n",
    "    elif sentence[0] == True:\n",
    "        return badword_find(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kss\n",
    "from pykospacing import Spacing\n",
    "from soynlp.normalizer import repeat_normalize\n",
    "\n",
    "# 띄어쓰기 설정\n",
    "spacing = Spacing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특수문자 제거\n",
    "def cleanse(text):\n",
    "    pattern = re.compile(r'\\s+')\n",
    "    text = re.sub(pattern, ' ', text)\n",
    "    text = re.sub('[^가-힣ㄱ-ㅎㅏ|a-zA-Z0-9]','', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'어바지가 방에 들어가신다'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_and_repeat_normalize(text):\n",
    "    cleansed_text = cleanse(text)                                             # 특수문자 제거\n",
    "    normalized_text = repeat_normalize(cleansed_text, num_repeats=2)          # 중복문자 제거\n",
    "    input_data = re.sub(r'\\d', '', normalized_text)                           #  제거\n",
    "    normalized_text = spacing(input_data)                                     # 띄어쓰기 보정 \n",
    "\n",
    "    return normalized_text\n",
    "\n",
    "clean_and_repeat_normalize(\"아버지가방에들어가신다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_output():\n",
    "    input_text = clean_and_repeat_normalize(input())\n",
    "    sentences = kss.split_sentences(input_text)\n",
    "\n",
    "    sentences_list = []\n",
    "    for sentence in sentences:\n",
    "        text_output = speak(sentence)\n",
    "        sentences_list.append(text_output)\n",
    "\n",
    "    long_test = ' '.join(sentences_list)\n",
    "    print(long_test)\n",
    "    return long_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지미랄\n",
      "난리\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'난리'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nv37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
