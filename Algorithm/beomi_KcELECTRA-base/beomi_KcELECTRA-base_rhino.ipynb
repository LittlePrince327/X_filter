{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# beomi/KcELECTRA-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 1835692544820087483\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 2254123828\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 9169485238189335735\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1650 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "라이브러리 설치가 완료되었습니다.\n",
      "absl-py==2.0.0\n",
      "accelerate==0.20.1\n",
      "annotated-types==0.6.0\n",
      "asttokens==2.4.1\n",
      "astunparse==1.6.3\n",
      "backcall==0.2.0\n",
      "blis==0.7.11\n",
      "cachetools==5.3.2\n",
      "catalogue==2.0.10\n",
      "certifi==2023.7.22\n",
      "chardet==5.2.0\n",
      "charset-normalizer==3.3.1\n",
      "click==8.1.7\n",
      "cloudpathlib==0.16.0\n",
      "colorama==0.4.6\n",
      "comm==0.1.4\n",
      "confection==0.1.3\n",
      "contourpy==1.1.1\n",
      "cycler==0.12.1\n",
      "cymem==2.0.8\n",
      "dataclasses==0.6\n",
      "debugpy==1.8.0\n",
      "decorator==5.1.1\n",
      "emoji==1.2.0\n",
      "en-core-web-sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.0/en_core_web_sm-3.7.0-py3-none-any.whl#sha256=6215d71a3212690e9aec49408a27e3fe6ad7cd6c715476e93d70dc784041e93e\n",
      "et-xmlfile==1.1.0\n",
      "exceptiongroup==1.1.3\n",
      "executing==2.0.0\n",
      "filelock==3.12.4\n",
      "flatbuffers==1.12\n",
      "fonttools==4.43.1\n",
      "fsspec==2023.10.0\n",
      "gast==0.4.0\n",
      "google-auth==2.23.3\n",
      "google-auth-oauthlib==0.4.6\n",
      "google-pasta==0.2.0\n",
      "grpcio==1.59.0\n",
      "h5py==3.1.0\n",
      "huggingface-hub==0.17.3\n",
      "idna==3.4\n",
      "importlib-metadata==6.8.0\n",
      "importlib-resources==6.1.0\n",
      "iniconfig==2.0.0\n",
      "install==1.3.5\n",
      "ipykernel==6.26.0\n",
      "ipython==8.12.3\n",
      "jedi==0.19.1\n",
      "Jinja2==3.1.2\n",
      "joblib==1.3.2\n",
      "JPype1==1.4.1\n",
      "jupyter_client==8.5.0\n",
      "jupyter_core==5.4.0\n",
      "keras==2.9.0\n",
      "Keras-Preprocessing==1.1.2\n",
      "kiwisolver==1.4.5\n",
      "konlpy==0.6.0\n",
      "Korpora==0.2.0\n",
      "kss==4.5.4\n",
      "langcodes==3.3.0\n",
      "libclang==16.0.6\n",
      "lxml==4.9.3\n",
      "Markdown==3.5\n",
      "MarkupSafe==2.1.3\n",
      "matplotlib==3.7.3\n",
      "matplotlib-inline==0.1.6\n",
      "mpmath==1.3.0\n",
      "murmurhash==1.0.10\n",
      "nest-asyncio==1.5.8\n",
      "networkx==3.1\n",
      "nltk==3.8.1\n",
      "numpy==1.24.4\n",
      "oauthlib==3.2.2\n",
      "openpyxl==3.1.2\n",
      "opt-einsum==3.3.0\n",
      "packaging==23.2\n",
      "pandas==2.0.3\n",
      "parso==0.8.3\n",
      "pecab==1.0.8\n",
      "pickleshare==0.7.5\n",
      "Pillow==10.1.0\n",
      "platformdirs==3.11.0\n",
      "pluggy==1.3.0\n",
      "preshed==3.0.9\n",
      "prompt-toolkit==3.0.39\n",
      "protobuf==3.19.6\n",
      "psutil==5.9.6\n",
      "pure-eval==0.2.2\n",
      "pyarrow==14.0.0\n",
      "pyasn1==0.5.0\n",
      "pyasn1-modules==0.3.0\n",
      "pydantic==2.4.2\n",
      "pydantic_core==2.10.1\n",
      "Pygments==2.16.1\n",
      "pykospacing @ git+https://github.com/haven-jeon/PyKoSpacing.git@d1e2f93759b1bcc74b6aaa345a62c08701e5546e\n",
      "pyparsing==3.1.1\n",
      "pytest==7.4.3\n",
      "python-dateutil==2.8.2\n",
      "pytz==2023.3.post1\n",
      "pywin32==306\n",
      "PyYAML==6.0.1\n",
      "pyzmq==25.1.1\n",
      "regex==2023.10.3\n",
      "requests==2.31.0\n",
      "requests-oauthlib==1.3.1\n",
      "rhinoMorph==4.0.1.12\n",
      "rsa==4.9\n",
      "safetensors==0.4.0\n",
      "scikit-learn==1.3.2\n",
      "scipy==1.10.1\n",
      "six==1.16.0\n",
      "smart-open==6.4.0\n",
      "soynlp==0.0.493\n",
      "spacy==3.7.2\n",
      "spacy-legacy==3.0.12\n",
      "spacy-loggers==1.0.5\n",
      "srsly==2.4.8\n",
      "stack-data==0.6.3\n",
      "sympy==1.12\n",
      "tensorboard==2.9.1\n",
      "tensorboard-data-server==0.6.1\n",
      "tensorboard-plugin-wit==1.8.1\n",
      "tensorflow==2.9.3\n",
      "tensorflow-estimator==2.9.0\n",
      "tensorflow-gpu==2.10.0\n",
      "tensorflow-io-gcs-filesystem==0.31.0\n",
      "termcolor==2.3.0\n",
      "thinc==8.2.1\n",
      "threadpoolctl==3.2.0\n",
      "tokenizers==0.13.3\n",
      "tomli==2.0.1\n",
      "torch==2.1.0\n",
      "tornado==6.3.3\n",
      "tqdm==4.66.1\n",
      "traitlets==5.12.0\n",
      "transformers==4.30.2\n",
      "typer==0.9.0\n",
      "typing_extensions==4.8.0\n",
      "tzdata==2023.3\n",
      "urllib3==2.0.7\n",
      "wasabi==1.1.2\n",
      "wcwidth==0.2.8\n",
      "weasel==0.3.3\n",
      "Werkzeug==3.0.1\n",
      "wordcloud==1.9.2\n",
      "wrapt==1.15.0\n",
      "xlrd==2.0.1\n",
      "zipp==3.17.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -eras (c:\\users\\gjaischool\\.conda\\envs\\nv38\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%run KcELECTRA-base.version.py\n",
    "!pip freeze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "devcie: cpu\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# deivce 선택\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"devcie:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\GJAISCHOOL\\\\Desktop\\\\X_filter\\\\Algorithm\\\\dataset\\\\asdf.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\GJAISCHOOL\\Desktop\\X_filter\\Algorithm\\beomi_KcELECTRA-base\\beomi_KcELECTRA-base_rhino.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/GJAISCHOOL/Desktop/X_filter/Algorithm/beomi_KcELECTRA-base/beomi_KcELECTRA-base_rhino.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# 엑셀 파일에서 데이터프레임 읽기\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/GJAISCHOOL/Desktop/X_filter/Algorithm/beomi_KcELECTRA-base/beomi_KcELECTRA-base_rhino.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_excel(\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC:\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mUsers\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mGJAISCHOOL\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mDesktop\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mX_filter\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mAlgorithm\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mdataset\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39masdf.xlsx\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/GJAISCHOOL/Desktop/X_filter/Algorithm/beomi_KcELECTRA-base/beomi_KcELECTRA-base_rhino.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# 'Sentence' 칼럼의 값을 문자열로 변환하여 리스트로 저장\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/GJAISCHOOL/Desktop/X_filter/Algorithm/beomi_KcELECTRA-base/beomi_KcELECTRA-base_rhino.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m str_data \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mSentence\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mastype(\u001b[39mstr\u001b[39m)\u001b[39m.\u001b[39mtolist()\n",
      "File \u001b[1;32mc:\\Users\\GJAISCHOOL\\.conda\\envs\\nv38\\lib\\site-packages\\pandas\\io\\excel\\_base.py:478\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(io, ExcelFile):\n\u001b[0;32m    477\u001b[0m     should_close \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 478\u001b[0m     io \u001b[39m=\u001b[39m ExcelFile(io, storage_options\u001b[39m=\u001b[39;49mstorage_options, engine\u001b[39m=\u001b[39;49mengine)\n\u001b[0;32m    479\u001b[0m \u001b[39melif\u001b[39;00m engine \u001b[39mand\u001b[39;00m engine \u001b[39m!=\u001b[39m io\u001b[39m.\u001b[39mengine:\n\u001b[0;32m    480\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    481\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mEngine should not be specified when passing \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    482\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    483\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\GJAISCHOOL\\.conda\\envs\\nv38\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1496\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[0;32m   1494\u001b[0m     ext \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxls\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1495\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1496\u001b[0m     ext \u001b[39m=\u001b[39m inspect_excel_format(\n\u001b[0;32m   1497\u001b[0m         content_or_path\u001b[39m=\u001b[39;49mpath_or_buffer, storage_options\u001b[39m=\u001b[39;49mstorage_options\n\u001b[0;32m   1498\u001b[0m     )\n\u001b[0;32m   1499\u001b[0m     \u001b[39mif\u001b[39;00m ext \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1500\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1501\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mExcel file format cannot be determined, you must specify \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1502\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39man engine manually.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1503\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\GJAISCHOOL\\.conda\\envs\\nv38\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1371\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1368\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(content_or_path, \u001b[39mbytes\u001b[39m):\n\u001b[0;32m   1369\u001b[0m     content_or_path \u001b[39m=\u001b[39m BytesIO(content_or_path)\n\u001b[1;32m-> 1371\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n\u001b[0;32m   1372\u001b[0m     content_or_path, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m, storage_options\u001b[39m=\u001b[39;49mstorage_options, is_text\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[0;32m   1373\u001b[0m ) \u001b[39mas\u001b[39;00m handle:\n\u001b[0;32m   1374\u001b[0m     stream \u001b[39m=\u001b[39m handle\u001b[39m.\u001b[39mhandle\n\u001b[0;32m   1375\u001b[0m     stream\u001b[39m.\u001b[39mseek(\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\GJAISCHOOL\\.conda\\envs\\nv38\\lib\\site-packages\\pandas\\io\\common.py:868\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    859\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\n\u001b[0;32m    860\u001b[0m             handle,\n\u001b[0;32m    861\u001b[0m             ioargs\u001b[39m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    864\u001b[0m             newline\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(handle, ioargs\u001b[39m.\u001b[39;49mmode)\n\u001b[0;32m    869\u001b[0m     handles\u001b[39m.\u001b[39mappend(handle)\n\u001b[0;32m    871\u001b[0m \u001b[39m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\GJAISCHOOL\\\\Desktop\\\\X_filter\\\\Algorithm\\\\dataset\\\\asdf.xlsx'"
     ]
    }
   ],
   "source": [
    "# 엑셀 파일에서 데이터프레임 읽기\n",
    "df = pd.read_excel(r\"C:\\Users\\GJAISCHOOL\\Desktop\\X_filter\\Algorithm\\dataset\\asdf.xlsx\")\n",
    "\n",
    "# 'Sentence' 칼럼의 값을 문자열로 변환하여 리스트로 저장\n",
    "str_data = df['Sentence'].astype(str).tolist()\n",
    "\n",
    "# 'Sentence' 칼럼의 값을 하나의 문자열로 결합\n",
    "all_sentences = ' '.join(str_data)\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_idx = df[df.label.isnull()].index                             # 해당 index에 null 값 확인\n",
    "df.loc[null_idx, \"Sentence\"]                                       # null 값이 존재한 인덱스의 content 값 불러오기\n",
    "\n",
    "# lable은 content의 가장 끝 문장열로 설정\n",
    "df.loc[null_idx, \"label\"] = df.loc[null_idx, \"Sentence\"].apply(lambda x: x[-1])\n",
    "\n",
    "# content는 \"\\t\" 앞부분까지의 문자열로 설정\n",
    "df.loc[null_idx, \"Sentence\"] = df.loc[null_idx, \"Sentence\"].apply(lambda x: x[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filepath:  C:\\Users\\GJAISCHOOL\\.conda\\envs\\nv38\\Lib\\site-packages\n",
      "classpath:  C:\\Users\\GJAISCHOOL\\.conda\\envs\\nv38\\Lib\\site-packages\\rhinoMorph/lib/rhino.jar\n",
      "RHINO started!\n",
      "['좌', '배다', '까다', 'ㅇ', 'ㅂ']\n",
      "['집', '롱', '패딩', '세', '더', '입다', 'ㅋㅋ']\n",
      "['개소리', '빨갱이', '옹호', '드루', '킹', 'ㅇㅇ', '짓다', '말다', '못하다', '삐다', '진거', '빨갱']\n",
      "['세탁', '되다']\n",
      "['애새끼', '초딩', 'ㅋㅋ', 'ㅋㅋ']\n",
      "['부대', '후예', '그렇다', '가학적', '아이디어', '세계', '최고', '이리하다', '애교', '떨다', '돈', '벌리다', '한국', '쓰다', '진출', '남자', '또', '이쁘다', '여자', '사족', '못쓰다', '공주대', '접하다', '놈']\n",
      "['재앙']\n",
      "['글쓴이', '와', '꾸다', '승리', '비하다', '방사능', '피폭', '원숭이']\n",
      "['마', '씨발', '아', '평이', '맷', '가다', '대하다', '햄', '해']\n",
      "['은행', '대출', '상담', '받다', '가보다', '직업', '귀천', '바로', '알리다']\n",
      "['ㅋㅋ', 'ㅋㅋ', 'ㅋ']\n",
      "['ㄹ', 'ㅇ', 'ㅋㅋ', 'ㅋ']\n",
      "['지역', '금태섭', '뽑다', '안되다', '그래도', '자한']\n",
      "['꽃다발', '사다', '들다', '알바', '하다', '곳', '찾아가다', '무릎', '꿇다', '마음', '받다', '지영아', '하다', '바로', '넘다', '여자', '이리하다', '존나', '좋아하다']\n",
      "['박근혜', '안', '빠', '보수', '통합', '원칙', '인정']\n",
      "['대가리', '필터', '없다', '연봉', '강사', '대가', '없다', '용접공']\n",
      "['백정주', '라', '고하다', '비하다', '느끼다', '본인', '직업', '자부심', '없다']\n",
      "['이것다', '다', '백인', '탓']\n",
      "['저러다', '골', '빌다', 'ㄴ', '다', '있다', 'ㅋㅋ', 'ㅋ', '저런', '학원', '강사', '애', '불쌍']\n",
      "['마감', '시간', '다', '되다', '떨이', '고기', '먹다', '식중독', '오래', '되다', '새', '거처', '럼', '둔갑', '다음날', '팔다', '테', '떨이', '개념', '아예', '없다']\n",
      "['열사', '들이키다', '아깝다']\n",
      "['정신병자', '천국']\n",
      "['으', '옷', '필요', '없다', '당', '깨다', '현금', '달랑', '깨다']\n",
      "[]\n",
      "['사이트', '아이디', '비번', '다', '다르다', '난이도', '높다', '음', '엑셀', '파일', '모든', '아이디', '비번', '계좌', '정보', '다', '들다', '고이다', '엑셀', '파일', '비번', '기억']\n",
      "['깍다', '새들', '시발', '존나', '강요', '하다']\n",
      "['리뷰', '쓰다', '새끼', '쓰레기', '맞다', 'ㅋㅋ', 'ㅋ', '존나', '악랄', '사진', '찍다', '올리다']\n",
      "['ㅋㅋ', 'ㅇ', 'ㅈ', '저런', '앉다', '감전사', '당하다']\n",
      "['틀다', '딱', '씨발새끼', '다', '꺼지다', '냄새나다']\n",
      "['근데', '요즘', '살짝', '트렌드', '지나다']\n",
      "['군대', '여', '군', '필요', '하다']\n",
      "['ㅋㅋ', '복수']\n",
      "['세금', '안내면', '같다', '동사무소', '프린트', '하다', '새끼', '월급', '못', '받다', '굶다', '뒈지다', '되다', '병신', '새끼']\n",
      "['면도', '무슨', '스머프', '모자', '쓰다', '고', '왓']\n",
      "['사형', '예지']\n",
      "['당연히', '많이', '보다', '밖']\n",
      "['흑', '종', '원', 'ㅋㅋ', 'ㅋㅋ', 'ㅋㅋ', 'ㅋㅋ']\n",
      "['원래', '수준', '그래']\n",
      "['호주', '남자', '동물', '못하다', '존재', 'ㅋㅋ']\n",
      "['나베', '나베', '하다']\n",
      "['전', '제주도', '여행', '가다', '폭포', '앞', '사진', '잘나다', '포토', '존', '사진', '찍다', '기다리다', '앞', '짱', '게이', '짜증', '남다', '먼저', '각자', '한장', '찍다', '음', '다음', '번갈다', '짝', '짓다', '찍다', '정말', '끝나다', '찍다', '그', '다', '모이다', '포즈', '바꾸다', '또', '사진', '찍다', '음', '이지', '하다']\n",
      "['행하다', '없다', '경건', '하다', '의롭다', '하다', '믿다', '사람', '믿음', '의', '여기다', '지느']\n",
      "['짱', '깨다', '말', '짱', '깨', '국', '무상', '의료']\n",
      "['빡', '대가리', '방법', '가르치다', 'ㅜㅜ']\n",
      "['누구']\n",
      "['시발', '정작', '지네', '자식', '몸', '쓰다', '일', '안시', '킬', '개지랄', '뜨다', 'ㅋㅋ', 'ㅋ']\n",
      "['용접', '협회', '무시', '말다', '현장', '고수', '및', '용접', '관하다', '전문', '지식', '가지다', '대학', '교수', '학자', '모임', '쎄다']\n",
      "['하다']\n",
      "['안', '하다', '애', '결국', '학업', '흥미', '못', '느끼다', '다']\n",
      "['틀다', '딱', '닭', '중국', '그때그때', '빨다', '새끼', '들이다', '닭', '중하다', '중국', '빨다', '오성홍', '기절', '억울', '논리적', '반박', '하다', 'ㅋ']\n",
      "['그냥', '홀딩', '하다']\n",
      "['만', '나이', '공식화']\n",
      "['꼴리다', '그냥', '존나', '귀엽다']\n",
      "['그냥', '선거철', '지지자', '결집', '시키다', '지지', '아시아인', '별로', '없다', '이민자', '싫어하다', '사람', '대부분']\n",
      "['오다', '마', '쥬', '무슨', '지다', '앎']\n",
      "['카다', '성공']\n",
      "['쓰레기', '휴지통']\n",
      "['크로', '보다', '야스키', '개털', '생각나다', 'ㅋㅋ', 'ㅋㅋ', 'ㅋㅋ', '진짜', '잡다', '잡다', '패다', 'ㅋㅋ', 'ㅋㅋ', '전직', '은행원', '보다', '야스키', 'ㅋ']\n",
      "['용팔이']\n",
      "['그렇다', '쑤시다', '박다']\n",
      "['그래서', '씨발', '롬', 'PC', '나쁘다', '좋다']\n",
      "['미치다', '진짜', '시발']\n",
      "['키', '사람', '끼', '리', '때', '누구', '키', '어', '내키다', '하다', '주변', '사람', '오오', '켜다', '재다', '하다', '켜다', '재다', '더', '크다', 'ㅋㅋ', 'ㅋ', '슬퍼하다', '모습', '개꿀잼']\n",
      "['아', '보다', '때다', '심장', '뭉클']\n",
      "['ㄹ', 'ㅇ', '시위', '진압', 'ㅆ', 'ㅅ', '음']\n",
      "['정재', '와', '그라다']\n",
      "['잘못', '하다', '같다', '문희상', '천황', '립', '소리', '문희상', '천황', '말해', '잘못', '하다', '애초', '일', '왕', '지금', '천황', '전범', '아들', '위안부', '직접', '사과', '말', '불러오다', '파장', '생각', '못하다', '경솔', '대처', '하다', '빌미', '말']\n",
      "['진짜', '시간', '하다', '정도', '히로뽕', '맞다', '있다', '경교대', '때', '뽕', '재소자', '의무실', '가다', '계호', '때', '내보다', '히로뽕', '하다', '단순', '기분', '좋다', '목적', 'ㅅ', 'ㅅ', '비아그라', '효과', '좋다', 'ㅋㅋ', 'ㅋ']\n",
      "['휴게소', '마트', '여성', '전용', '주차장', '보다', '분노', '하다']\n",
      "['지르다', '끝', '보다', 'ㅆ', 'ㅂ', '만나다', '자해', '이야기', '머리채', '잡다', '좋다', '하다', '안되다', '그담', '허리띠', '풀다']\n",
      "['앞머리', '좀', '자르다', 'ㅅㅂ']\n",
      "['아', '그렇다', 'ㅋㅋ', 'ㅋ']\n",
      "['영혼', '까지다', '되다', '놈', '일거']\n",
      "['그때', '조언', '멱살', '잡', 'ㅋㅋ']\n",
      "['남친']\n",
      "['진주', '여기', '동네', '잠시', '살다', '여기', '주차', '장난', '그냥', '욕', '나오다']\n",
      "['대구경', '북', '당선', '조건', '매국노']\n",
      "['찐따', '답', 'ㅋㅋ', 'ㅋㅋ', 'ㅋㅋ']\n",
      "['노가다', '다니다', '싫다', '노력', '살지다', '그리하다', '틀다', '딱', 'ㅋㅋ', '어리다', '꿈', '용접', '상황', 'ㅎ', 'ㅌ', 'ㅊ', '되다', '그나마', '하다', '용접', '하다', '돈', '되다', '존나', '힘들다', '보람', '느끼다', '사람', '되다', '느끼다', '보다', '노가다', '노가다', '주제', '알다', '살다']\n",
      "['피파', '회장', '보내다']\n",
      "['우리나라', '남성', '집해', '유예', '엄마', '때다', '릴', '같다', '아', '님', '말다']\n",
      "['와', '석유', '나라', '위엄', '바로', '저런', '거로', '근데', '저런', '선진국', '되다', '나라', '일부', '새끼', '그', '부르다', '독점', '차도르', '씌우다', '전쟁', '놀음', '하다', '여']\n",
      "['가다', '정도', '차라리', '걍', '호텔', '뷔페', '감호', '뷔페', '랍스터', '없다', '그냥']\n",
      "['안동', '김']\n",
      "['새로', '다', '그렇게', '하다', '열심히', '하다', '티', '팍팍', '바꾸다', '뭐', '하다', '압박', '주다', '너무', '타성', '젖다', '위기감', '없이']\n",
      "['ㅂㄷㅂㄷ', 'ㅋㅋ', 'ㅋㅋ', 'ㅋ', '중국', '욕하다', 'ㅂㄷㅂㄷ', '하다', 'ㅋㅋ', 'ㅋㅋ', 'ㅋㅋ', 'ㅋㅋ']\n",
      "['정치인', '위', '문재']\n",
      "['오이', '오이', '아부']\n",
      "['무게', '치', '인생', '버리다', '새끼']\n",
      "['빌다', '런', '런']\n",
      "['안비']\n",
      "['가세', '연', '정규', '위장보', '분탕', 'ㅋㅋ', 'ㅋㅋ', 'ㅋ']\n",
      "['수술', '남자', '의사', '잘', '하다']\n",
      "['이봐요', '미친놈']\n",
      "['그러나', '수치', '상일', '현실', '굉장히', '망하다', '가능성', '높다', '나라']\n",
      "['여자', '물뽕', '처먹이다', '강간', '동영상', '찍다', '죄', 'ㅋㅋ', '버닝', '썰다', '중국', '삼다', '합', '회', '자금', '세탁소']\n"
     ]
    }
   ],
   "source": [
    "import rhinoMorph\n",
    "rn = rhinoMorph.startRhino()\n",
    "\n",
    "# 'Sentence' 열에 형태소 분석 적용\n",
    "for sentence in df['Sentence']:\n",
    "    result = rhinoMorph.onlyMorph_list(rn, sentence, pos=['NNG', 'NNP', 'VV', 'VA', 'XR', 'IC', 'MM', 'MAG', 'MAJ'], eomi=True)\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list into a Series\n",
    "result_series = pd.Series(result)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data = result_series.sample(frac=0.8, random_state=42)\n",
    "test_data = result_series.drop(train_data.index)\n",
    "\n",
    "train_data_list = train_data.tolist()\n",
    "test_data_list = test_data.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 데이터를 훈련 데이터와 테스트 데이터로 나누기\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)  # 여기서 0.2는 테스트 데이터의 비율을 나타냅니다.\n",
    "\n",
    "# 'Sentence' 열의 데이터를 형태소 분석 적용\n",
    "train_data['result'] = train_data['Sentence'].apply(lambda sentence: rhinoMorph.onlyMorph_list(rn, sentence, pos=['NNG', 'NNP', 'VV', 'VA', 'XR', 'IC', 'MM', 'MAG', 'MAJ'], eomi=True))\n",
    "test_data['result'] = test_data['Sentence'].apply(lambda sentence: rhinoMorph.onlyMorph_list(rn, sentence, pos=['NNG', 'NNP', 'VV', 'VA', 'XR', 'IC', 'MM', 'MAG', 'MAJ'], eomi=True))\n",
    "train_data['result_str'] = train_data['result'].apply(lambda x: ' '.join(x))\n",
    "test_data['result_str'] = test_data['result'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\GJAISCHOOL\\.conda\\envs\\nv38\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import ElectraTokenizer, ElectraForSequenceClassification, TFElectraForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "\n",
    "# KcELECTRA 모델 및 토크나이저 로드\n",
    "model_name = \"beomi/KcELECTRA-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>result</th>\n",
       "      <th>result_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>전에 제주도 여행 갔는데 폭포 앞에 사진 잘나오는 포토존에서 사진 찍는거 기다렸는데...</td>\n",
       "      <td>1</td>\n",
       "      <td>[전, 제주도, 여행, 가다, 폭포, 앞, 사진, 잘나다, 포토, 존, 사진, 찍다...</td>\n",
       "      <td>전 제주도 여행 가다 폭포 앞 사진 잘나다 포토 존 사진 찍다 기다리다 앞 짱 게이...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>진짜 몇시간씩 할 정도면 히로뽕맞고 하는거일수도있다경교대있을때 뽕쟁이 재소자 의무실...</td>\n",
       "      <td>1</td>\n",
       "      <td>[진짜, 시간, 하다, 정도, 히로뽕, 맞다, 있다, 경교대, 때, 뽕, 재소자, ...</td>\n",
       "      <td>진짜 시간 하다 정도 히로뽕 맞다 있다 경교대 때 뽕 재소자 의무실 가다 계호 때 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>대가리에 필터없는 연봉 30억 강사vs대가리가 없는 용접공</td>\n",
       "      <td>1</td>\n",
       "      <td>[대가리, 필터, 없다, 연봉, 강사, 대가, 없다, 용접공]</td>\n",
       "      <td>대가리 필터 없다 연봉 강사 대가 없다 용접공</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>휴게소, 마트 등의 여성전용주차장 보고 분노를 해야지</td>\n",
       "      <td>0</td>\n",
       "      <td>[휴게소, 마트, 여성, 전용, 주차장, 보다, 분노, 하다]</td>\n",
       "      <td>휴게소 마트 여성 전용 주차장 보다 분노 하다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>무게치는거에 인생버린새끼들</td>\n",
       "      <td>1</td>\n",
       "      <td>[무게, 치, 인생, 버리다, 새끼]</td>\n",
       "      <td>무게 치 인생 버리다 새끼</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>그래서 씨발롬아 PC가 나쁘다는 거야 좋다는 거야?</td>\n",
       "      <td>1</td>\n",
       "      <td>[그래서, 씨발, 롬, PC, 나쁘다, 좋다]</td>\n",
       "      <td>그래서 씨발 롬 PC 나쁘다 좋다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>아 그런거야? ㅋㅋㅋ</td>\n",
       "      <td>0</td>\n",
       "      <td>[아, 그렇다, ㅋㅋ, ㅋ]</td>\n",
       "      <td>아 그렇다 ㅋㅋ ㅋ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>박근혜 안빠는데 보수통합 3원칙 인정함</td>\n",
       "      <td>1</td>\n",
       "      <td>[박근혜, 안, 빠, 보수, 통합, 원칙, 인정]</td>\n",
       "      <td>박근혜 안 빠 보수 통합 원칙 인정</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>수술도 남자 의사가 잘한다</td>\n",
       "      <td>0</td>\n",
       "      <td>[수술, 남자, 의사, 잘, 하다]</td>\n",
       "      <td>수술 남자 의사 잘 하다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>만 나이 공식화도</td>\n",
       "      <td>0</td>\n",
       "      <td>[만, 나이, 공식화]</td>\n",
       "      <td>만 나이 공식화</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Sentence  label  \\\n",
       "40  전에 제주도 여행 갔는데 폭포 앞에 사진 잘나오는 포토존에서 사진 찍는거 기다렸는데...      1   \n",
       "67  진짜 몇시간씩 할 정도면 히로뽕맞고 하는거일수도있다경교대있을때 뽕쟁이 재소자 의무실...      1   \n",
       "15                   대가리에 필터없는 연봉 30억 강사vs대가리가 없는 용접공      1   \n",
       "68                      휴게소, 마트 등의 여성전용주차장 보고 분노를 해야지      0   \n",
       "88                                     무게치는거에 인생버린새끼들      1   \n",
       "..                                                ...    ...   \n",
       "60                       그래서 씨발롬아 PC가 나쁘다는 거야 좋다는 거야?      1   \n",
       "71                                        아 그런거야? ㅋㅋㅋ      0   \n",
       "14                              박근혜 안빠는데 보수통합 3원칙 인정함      1   \n",
       "92                                     수술도 남자 의사가 잘한다      0   \n",
       "51                                          만 나이 공식화도      0   \n",
       "\n",
       "                                               result  \\\n",
       "40  [전, 제주도, 여행, 가다, 폭포, 앞, 사진, 잘나다, 포토, 존, 사진, 찍다...   \n",
       "67  [진짜, 시간, 하다, 정도, 히로뽕, 맞다, 있다, 경교대, 때, 뽕, 재소자, ...   \n",
       "15                 [대가리, 필터, 없다, 연봉, 강사, 대가, 없다, 용접공]   \n",
       "68                 [휴게소, 마트, 여성, 전용, 주차장, 보다, 분노, 하다]   \n",
       "88                               [무게, 치, 인생, 버리다, 새끼]   \n",
       "..                                                ...   \n",
       "60                          [그래서, 씨발, 롬, PC, 나쁘다, 좋다]   \n",
       "71                                    [아, 그렇다, ㅋㅋ, ㅋ]   \n",
       "14                        [박근혜, 안, 빠, 보수, 통합, 원칙, 인정]   \n",
       "92                                [수술, 남자, 의사, 잘, 하다]   \n",
       "51                                       [만, 나이, 공식화]   \n",
       "\n",
       "                                           result_str  \n",
       "40  전 제주도 여행 가다 폭포 앞 사진 잘나다 포토 존 사진 찍다 기다리다 앞 짱 게이...  \n",
       "67  진짜 시간 하다 정도 히로뽕 맞다 있다 경교대 때 뽕 재소자 의무실 가다 계호 때 ...  \n",
       "15                          대가리 필터 없다 연봉 강사 대가 없다 용접공  \n",
       "68                          휴게소 마트 여성 전용 주차장 보다 분노 하다  \n",
       "88                                     무게 치 인생 버리다 새끼  \n",
       "..                                                ...  \n",
       "60                                 그래서 씨발 롬 PC 나쁘다 좋다  \n",
       "71                                         아 그렇다 ㅋㅋ ㅋ  \n",
       "14                                박근혜 안 빠 보수 통합 원칙 인정  \n",
       "92                                      수술 남자 의사 잘 하다  \n",
       "51                                           만 나이 공식화  \n",
       "\n",
       "[76 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_sentences = tokenizer(\n",
    "    list(train_data[\"result_str\"]),\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=128,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    add_special_tokens=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test_sentences = tokenizer(\n",
    "    list(test_data[\"Sentence\"]),\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=128,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    add_special_tokens=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding(num_tokens=59, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
      "['ĠìłĦ', 'Ġìłľì£¼ëıĦ', 'ĠìĹ¬íĸī', 'Ġê°Ģ', 'ëĭ¤', 'ĠíıŃ', 'íı¬', 'Ġìķŀ', 'ĠìĤ¬ì§Ħ', 'Ġìŀĺ', 'ëĤĺëĭ¤', 'Ġíı¬íĨł', 'Ġì¡´', 'ĠìĤ¬ì§Ħ', 'Ġì°į', 'ëĭ¤', 'Ġê¸°ëĭ¤', 'ë¦¬ëĭ¤', 'Ġìķŀ', 'Ġì§±', 'Ġê²ĮìĿ´', 'Ġì§ľì¦Ŀ', 'ĠëĤ¨', 'ëĭ¤', 'Ġë¨¼ìłĢ', 'Ġê°ģìŀĲ', 'Ġíķľìŀ¥', 'Ġì°į', 'ëĭ¤', 'ĠìĿĮ', 'Ġëĭ¤ìĿĮ', 'Ġë²Ī', 'ê°Ī', 'ëĭ¤', 'Ġì§Ŀ', 'Ġì§ĵ', 'ëĭ¤', 'Ġì°į', 'ëĭ¤', 'Ġìłķë§Ĳ', 'ĠëģĿëĤĺ', 'ëĭ¤', 'Ġì°į', 'ëĭ¤', 'Ġê·¸', 'Ġëĭ¤', 'Ġëª¨', 'ìĿ´ëĭ¤', 'Ġíı¬', 'ì¦Ī', 'Ġë°Ķê¾¸', 'ëĭ¤', 'ĠëĺĲ', 'ĠìĤ¬ì§Ħ', 'Ġì°į', 'ëĭ¤', 'ĠìĿĮ', 'ĠìĿ´ì§Ģ', 'Ġíķĺëĭ¤']\n",
      "[502, 7050, 3994, 389, 237, 1283, 1520, 1124, 2519, 500, 9178, 26272, 1790, 2519, 1379, 237, 2846, 6527, 1124, 1762, 11350, 3706, 631, 237, 1858, 8840, 20856, 1379, 237, 1677, 1836, 2488, 1340, 237, 4745, 1381, 237, 1379, 237, 939, 5155, 237, 1379, 237, 434, 374, 463, 489, 1526, 2446, 3956, 237, 754, 2519, 1379, 237, 1677, 4798, 3088]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_sentences[0])        # 0번째 문장에 해당하는 객체 \n",
    "print(tokenized_train_sentences[0].tokens) # 0번째 문장에 토큰의 목록\n",
    "print(tokenized_train_sentences[0].ids)    # 0번째 문장에 대한 고유 ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CurseDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encoding, labels):\n",
    "        self.encodings = encoding\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()} # self.encodings 딕셔너리 내의 값 중에 val를 torch.tensor로 변환해 하여\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])                             # key: toch.tensor(val[ix])라는 item 딕셔너리 형성\n",
    "        return item                                                                 # 새로운 labels 키 값에 value torch.tensor(self.labels[idx]) 쌍을 생성\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)                                                     # self.labels 길이 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set, test_set에 대한 데이터셋을 각각 생성\n",
    "\n",
    "train_label = train_data[\"label\"].values\n",
    "test_label = test_data[\"label\"].values\n",
    "\n",
    "train_dataset = CurseDataset(tokenized_train_sentences, train_label)\n",
    "test_dataset = CurseDataset(tokenized_test_sentences, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at beomi/KcELECTRA-base were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at beomi/KcELECTRA-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ElectraForSequenceClassification(\n",
       "  (electra): ElectraModel(\n",
       "    (embeddings): ElectraEmbeddings(\n",
       "      (word_embeddings): Embedding(30000, 768, padding_idx=3)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): ElectraEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): ElectraClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)   # 사전 학습된 모델 찾아오기\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - 2 학습 파라미터 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = './results',           # 학습결과 저장경로\n",
    "    num_train_epochs=10,                # 학습 epoch 설정\n",
    "    per_device_train_batch_size = 8,    # train batch_size 설정\n",
    "    per_device_eval_batch_size = 64,    # test batch_size 설정\n",
    "    logging_dir = './logs',             # 학습 log 저장경로\n",
    "    logging_steps=100,                  # 학습 log 기록 단위\n",
    "    save_total_limit = 2,               # 학습결과 저장 최대갯수\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "# 학습과정에서 사용할 평가지표를 위한 함수 설정\n",
    "def compute_metrics(pred):\n",
    "  labels = pred.label_ids\n",
    "  preds = pred.predictions.argmax(-1)\n",
    "  # 정밀도, 재현율, f1 구하기 \n",
    "  precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "  # 정확도 구하기\n",
    "  acc = accuracy_score(labels, preds)\n",
    "  return{\n",
    "      'accuracy': acc,\n",
    "      'f1': f1,\n",
    "      'precision': precision,\n",
    "      'recall': recall\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer 모듈을 사용해 모델의 학습을 컨트롤하은 trainer를 생성\n",
    "trainer = Trainer(\n",
    "    model = model,                       # 학습하고자하는 Transformers model\n",
    "    args=training_args,                  # 위에서 정의한 Trainging Arguments\n",
    "    train_dataset=train_dataset,         # 학습 데이터셋\n",
    "    eval_dataset=test_dataset,           # 평가 데이터셋\n",
    "    compute_metrics=compute_metrics,     # 평가지표\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\GJAISCHOOL\\.conda\\envs\\nv38\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]C:\\Users\\GJAISCHOOL\\AppData\\Local\\Temp\\ipykernel_12548\\1167134061.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()} # self.encodings 딕셔너리 내의 값 중에 val를 torch.tensor로 변환해 하여\n",
      "100%|██████████| 100/100 [03:11<00:00,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3424, 'learning_rate': 0.0, 'epoch': 10.0}\n",
      "{'train_runtime': 191.4621, 'train_samples_per_second': 3.969, 'train_steps_per_second': 0.522, 'train_loss': 0.3423833465576172, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=0.3423833465576172, metrics={'train_runtime': 191.4621, 'train_samples_per_second': 3.969, 'train_steps_per_second': 0.522, 'train_loss': 0.3423833465576172, 'epoch': 10.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GJAISCHOOL\\AppData\\Local\\Temp\\ipykernel_12548\\1167134061.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()} # self.encodings 딕셔너리 내의 값 중에 val를 torch.tensor로 변환해 하여\n",
      "100%|██████████| 1/1 [00:00<00:00, 82.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'custom__loss': 0.45376983284950256, 'custom__accuracy': 0.8, 'custom__f1': 0.7999999999999999, 'custom__precision': 0.7272727272727273, 'custom__recall': 0.8888888888888888, 'custom__runtime': 0.4949, 'custom__samples_per_second': 40.415, 'custom__steps_per_second': 2.021, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 평가를 수행하고 custom_ 접두사가 붙은 평가 지표를 출력\n",
    "results = trainer.evaluate(eval_dataset=test_dataset, metric_key_prefix='custom_')\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, tensor([[2295,  640, 3898, 5170,   33]]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0: curse, 1: non_curse\n",
    "def sentence_predict(sent):\n",
    "    # 평가모드로 변경\n",
    "    model.eval()\n",
    "\n",
    "    # 입력된 문장 토크나이징\n",
    "    tokenized_sent = tokenizer(\n",
    "        sent,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "    # 모델이 위치한 GPU로 이동\n",
    "    # tokenized_sent.to(deivce)\n",
    "\n",
    "    # 예측\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=tokenized_sent[\"input_ids\"],\n",
    "            attention_mask=tokenized_sent[\"attention_mask\"],\n",
    "            token_type_ids=tokenized_sent[\"token_type_ids\"]\n",
    "        )\n",
    "\n",
    "    # 결과 return\n",
    "    logits = outputs[0]\n",
    "    logits = logits.detach().cpu()\n",
    "    result = logits.argmax(-1)\n",
    "    sentence = True\n",
    "    input_sentence = tokenized_sent[\"input_ids\"]\n",
    "    if result == 0:\n",
    "        sentence = True\n",
    "\n",
    "    elif result == 1:\n",
    "        sentence = False\n",
    "    return sentence, input_sentence\n",
    "\n",
    "\n",
    "sentence_predict(\"씨발이게 맞아?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입\n"
     ]
    }
   ],
   "source": [
    "def found_word(input_sentence, found_bad_word):\n",
    "    result = input_sentence\n",
    "    if found_bad_word:\n",
    "        result = badword_find(result)\n",
    "    return result\n",
    "\n",
    "def badword_find(input_sentence):\n",
    "    result = input_sentence\n",
    "    badword_df = pd.read_excel(r'C:\\Users\\GJAISCHOOL\\Desktop\\X_filter\\Algorithm\\dataset\\word_list.xlsx')\n",
    "    \n",
    "    found_bad_word = False  # 입력 문장에 단어가 발견되었는지를 나타내는 플래그\n",
    "    for idx, row in badword_df.iterrows():\n",
    "        if row[\"WORD\"] in input_sentence:\n",
    "            # 'WORD'가 입력 문장에 포함된 경우\n",
    "            new_word = row[\"대체어\"]\n",
    "            if not pd.isnull(new_word):\n",
    "                result = result.replace(row[\"WORD\"], new_word)\n",
    "                found_bad_word = True\n",
    "                break  # 대체어를 찾았으므로 반복문 종료\n",
    "            else:\n",
    "                result = result.replace(row[\"WORD\"], \"*\" * len(row[\"WORD\"]))\n",
    "                found_bad_word = True\n",
    "    \n",
    "    if not found_bad_word:\n",
    "        # result = \"@\" * len(input_sentence)\n",
    "        result = \"혐오 표현입니다.\"\n",
    "    return result\n",
    "\n",
    "# 테스트\n",
    "input_sentence = \"아가리\"\n",
    "speak_result = badword_find(input_sentence)\n",
    "print(speak_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'입'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "badword_find(input_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speak(sent):\n",
    "    sentence = sentence_predict(sent)\n",
    "    if sentence[0] == False:\n",
    "        return sent\n",
    "    elif sentence[0] == True:\n",
    "        return badword_find(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kss\n",
    "from pykospacing import Spacing\n",
    "from soynlp.normalizer import repeat_normalize\n",
    "\n",
    "# 띄어쓰기 설정\n",
    "spacing = Spacing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특수문자 제거\n",
    "def cleanse(text):\n",
    "    pattern = re.compile(r'\\s+')\n",
    "    text = re.sub(pattern, ' ', text)\n",
    "    text = re.sub('[^가-힣ㄱ-ㅎㅏ|a-zA-Z0-9]','', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'어바지가 방에 들어가신다'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_and_repeat_normalize(text):\n",
    "    cleansed_text = cleanse(text)                                             # 특수문자 제거\n",
    "    normalized_text = repeat_normalize(cleansed_text, num_repeats=2)          # 중복문자 제거\n",
    "    input_data = re.sub(r'\\d', '', normalized_text)                           # 숫자 제거\n",
    "    normalized_text = spacing(input_data)                                     # 띄어쓰기 보정 \n",
    "\n",
    "    return normalized_text\n",
    "\n",
    "clean_and_repeat_normalize(\"어바지가방에들어가신다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_output():\n",
    "    input_text = clean_and_repeat_normalize(input())\n",
    "    sentences = kss.split_sentences(input_text)\n",
    "\n",
    "    sentences_list = []\n",
    "    for sentence in sentences:\n",
    "        text_output = speak(sentence)\n",
    "        sentences_list.append(text_output)\n",
    "        print(sentence)\n",
    "\n",
    "    long_test = ' '.join(sentences_list)\n",
    "    print(long_test)\n",
    "    return long_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지미랄\n",
      "난리\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'난리'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nv37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
