{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\GJAISCHOOL\\.conda\\envs\\nv38\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torch\n",
      "Version: 2.1.0\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3\n",
      "Location: c:\\users\\gjaischool\\.conda\\envs\\nv38\\lib\\site-packages\n",
      "Requires: filelock, fsspec, jinja2, networkx, sympy, typing-extensions\n",
      "Required-by: accelerate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -eras (c:\\users\\gjaischool\\.conda\\envs\\nv38\\lib\\site-packages)\n",
      "WARNING: Package(s) not found: #, 2.1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.30.2\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: c:\\users\\gjaischool\\.conda\\envs\\nv38\\lib\\site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -eras (c:\\users\\gjaischool\\.conda\\envs\\nv38\\lib\\site-packages)\n",
      "WARNING: Package(s) not found: #, 4.30.2\n"
     ]
    }
   ],
   "source": [
    "!pip show torch          # 2.1.0\n",
    "!pip show transformers   # 4.30.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (‚Ä¶)/main/tokenizer.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.83M/2.83M [00:01<00:00, 2.65MB/s]\n",
      "c:\\Users\\GJAISCHOOL\\.conda\\envs\\nv38\\lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\GJAISCHOOL\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading (‚Ä¶)lve/main/config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.00k/1.00k [00:00<00:00, 239kB/s]\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['‚ñÅÏïàÎÖï',\n",
       " 'Ìïò',\n",
       " 'ÏÑ∏',\n",
       " 'Ïöî.',\n",
       " '‚ñÅÌïúÍµ≠Ïñ¥',\n",
       " '‚ñÅG',\n",
       " 'P',\n",
       " 'T',\n",
       " '-2',\n",
       " '‚ñÅÏûÖ',\n",
       " 'ÎãàÎã§.',\n",
       " 'üò§',\n",
       " ':)',\n",
       " 'l^o']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Î™®Îç∏ Î∂àÎü¨Ïò§Í∏∞\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
    "bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
    "pad_token='<pad>', mask_token='<mask>')\n",
    "tokenizer.tokenize(\"ÏïàÎÖïÌïòÏÑ∏Ïöî. ÌïúÍµ≠Ïñ¥ GPT-2 ÏûÖÎãàÎã§.üò§:)l^o\")\n",
    "# ['‚ñÅÏïàÎÖï', 'Ìïò', 'ÏÑ∏', 'Ïöî.', '‚ñÅÌïúÍµ≠Ïñ¥', '‚ñÅG', 'P', 'T', '-2', '‚ñÅÏûÖ', 'ÎãàÎã§.', 'üò§', ':)', 'l^o']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading pytorch_model.bin: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 513M/513M [00:27<00:00, 18.8MB/s] \n"
     ]
    }
   ],
   "source": [
    "# Î™®Îç∏ ÌïôÏäµ\n",
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'ÌõÑ ÏßÑÏßú ÎÇúÎ¶¨ÌïòÎÑ§'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The following `model_kwargs` are not used by the model: ['bos_toekn_id'] (note: typos in the generate arguments will also show up in this list)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\GJAISCHOOL\\Desktop\\X_filter\\Algorithm\\KOGPT2\\KoGPT2 CODE.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/GJAISCHOOL/Desktop/X_filter/Algorithm/KOGPT2/KoGPT2%20CODE.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m input_ids \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mencode(text, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/GJAISCHOOL/Desktop/X_filter/Algorithm/KOGPT2/KoGPT2%20CODE.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m gen_ids \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(input_ids,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/GJAISCHOOL/Desktop/X_filter/Algorithm/KOGPT2/KoGPT2%20CODE.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                          max_length\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/GJAISCHOOL/Desktop/X_filter/Algorithm/KOGPT2/KoGPT2%20CODE.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                          repetition_penalty\u001b[39m=\u001b[39;49m\u001b[39m2.0\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/GJAISCHOOL/Desktop/X_filter/Algorithm/KOGPT2/KoGPT2%20CODE.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                          pad_token_id\u001b[39m=\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/GJAISCHOOL/Desktop/X_filter/Algorithm/KOGPT2/KoGPT2%20CODE.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                          eos_token_id\u001b[39m=\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/GJAISCHOOL/Desktop/X_filter/Algorithm/KOGPT2/KoGPT2%20CODE.ipynb#W4sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                          bos_toekn_id\u001b[39m=\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mbos_token_id,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/GJAISCHOOL/Desktop/X_filter/Algorithm/KOGPT2/KoGPT2%20CODE.ipynb#W4sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                          use_cache\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\GJAISCHOOL\\.conda\\envs\\nv38\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\GJAISCHOOL\\.conda\\envs\\nv38\\lib\\site-packages\\transformers\\generation\\utils.py:1271\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[0;32m   1269\u001b[0m model_kwargs \u001b[39m=\u001b[39m generation_config\u001b[39m.\u001b[39mupdate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# All unused kwargs must be model kwargs\u001b[39;00m\n\u001b[0;32m   1270\u001b[0m generation_config\u001b[39m.\u001b[39mvalidate()\n\u001b[1;32m-> 1271\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_model_kwargs(model_kwargs\u001b[39m.\u001b[39;49mcopy())\n\u001b[0;32m   1273\u001b[0m \u001b[39m# 2. Set generation parameters if not already defined\u001b[39;00m\n\u001b[0;32m   1274\u001b[0m logits_processor \u001b[39m=\u001b[39m logits_processor \u001b[39mif\u001b[39;00m logits_processor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m LogitsProcessorList()\n",
      "File \u001b[1;32mc:\\Users\\GJAISCHOOL\\.conda\\envs\\nv38\\lib\\site-packages\\transformers\\generation\\utils.py:1144\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_kwargs\u001b[1;34m(self, model_kwargs)\u001b[0m\n\u001b[0;32m   1141\u001b[0m         unused_model_args\u001b[39m.\u001b[39mappend(key)\n\u001b[0;32m   1143\u001b[0m \u001b[39mif\u001b[39;00m unused_model_args:\n\u001b[1;32m-> 1144\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1145\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe following `model_kwargs` are not used by the model: \u001b[39m\u001b[39m{\u001b[39;00munused_model_args\u001b[39m}\u001b[39;00m\u001b[39m (note: typos in the\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1146\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m generate arguments will also show up in this list)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1147\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: The following `model_kwargs` are not used by the model: ['bos_toekn_id'] (note: typos in the generate arguments will also show up in this list)"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "gen_ids = model.generate(input_ids,\n",
    "                         max_length=5,\n",
    "                         repetition_penalty=2.0,\n",
    "                         pad_token_id=tokenizer.pad_token_id,\n",
    "                         eos_token_id=tokenizer.eos_token_id,\n",
    "                         bos_toekn_id=tokenizer.bos_token_id,\n",
    "                         use_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÌõÑ ÏßÑÏßú ÎÇúÎ¶¨ÌïòÎÑ§Ïöî.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generated = tokenizer.decode(gen_ids[0])\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
