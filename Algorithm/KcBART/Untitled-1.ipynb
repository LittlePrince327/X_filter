{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KcBART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\gjaischool\\.conda\\envs\\nv38\\lib\\site-packages (4.30.2)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.35.0-py3-none-any.whl.metadata (123 kB)\n",
      "     ---------------------------------------- 0.0/123.1 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/123.1 kB ? eta -:--:--\n",
      "     --------- --------------------------- 30.7/123.1 kB 435.7 kB/s eta 0:00:01\n",
      "     -------------------------------------  122.9/123.1 kB 1.0 MB/s eta 0:00:01\n",
      "     ------------------------------------ 123.1/123.1 kB 902.6 kB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\gjaischool\\.conda\\envs\\nv38\\lib\\site-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\gjaischool\\.conda\\envs\\nv38\\lib\\site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\gjaischool\\.conda\\envs\\nv38\\lib\\site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\gjaischool\\.conda\\envs\\nv38\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\gjaischool\\.conda\\envs\\nv38\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\gjaischool\\.conda\\envs\\nv38\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\gjaischool\\.conda\\envs\\nv38\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
      "  Using cached tokenizers-0.14.1-cp38-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\gjaischool\\.conda\\envs\\nv38\\lib\\site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\gjaischool\\.conda\\envs\\nv38\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\gjaischool\\.conda\\envs\\nv38\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\gjaischool\\.conda\\envs\\nv38\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\gjaischool\\.conda\\envs\\nv38\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gjaischool\\.conda\\envs\\nv38\\lib\\site-packages (from requests->transformers) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gjaischool\\.conda\\envs\\nv38\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gjaischool\\.conda\\envs\\nv38\\lib\\site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gjaischool\\.conda\\envs\\nv38\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
      "   ---------------------------------------- 0.0/7.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/7.9 MB 6.9 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 0.8/7.9 MB 8.8 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 1.8/7.9 MB 14.3 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 2.2/7.9 MB 11.6 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 2.8/7.9 MB 11.7 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 3.1/7.9 MB 12.2 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 3.7/7.9 MB 11.8 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 4.1/7.9 MB 11.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 5.1/7.9 MB 12.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.4/7.9 MB 13.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.2/7.9 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.9/7.9 MB 13.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.9/7.9 MB 13.0 MB/s eta 0:00:00\n",
      "Using cached tokenizers-0.14.1-cp38-none-win_amd64.whl (2.2 MB)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.13.3\n",
      "    Uninstalling tokenizers-0.13.3:\n",
      "      Successfully uninstalled tokenizers-0.13.3\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.30.2\n",
      "    Uninstalling transformers-4.30.2:\n",
      "      Successfully uninstalled transformers-4.30.2\n",
      "Successfully installed tokenizers-0.14.1 transformers-4.35.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -eras (c:\\users\\gjaischool\\.conda\\envs\\nv38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -eras (c:\\users\\gjaischool\\.conda\\envs\\nv38\\lib\\site-packages)\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\GJAISCHOOL\\.conda\\envs\\nv38\\Lib\\site-packages\\~okenizers'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "LocalTokenNotFoundError",
     "evalue": "Token is required (`token=True`), but no token found. You need to provide a token or be logged in to Hugging Face with `huggingface-cli login` or `huggingface_hub.login`. See https://huggingface.co/settings/tokens.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLocalTokenNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32mUntitled-1.ipynb Cell 4\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W6sdW50aXRsZWQ%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoTokenizer, AutoModelForSeq2SeqLM\n\u001b[0;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W6sdW50aXRsZWQ%3D?line=2'>3</a>\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhyunwoongko/kcbart-base\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W6sdW50aXRsZWQ%3D?line=3'>4</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(model_name, use_auth_token\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W6sdW50aXRsZWQ%3D?line=5'>6</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForSeq2SeqLM\u001b[39m.\u001b[39mfrom_pretrained(model_name, use_auth_token\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W6sdW50aXRsZWQ%3D?line=7'>8</a>\u001b[0m input_sentence \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m한국어 문장 생성 테스트입니다.\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\GJAISCHOOL\\.conda\\envs\\nv38\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:643\u001b[0m, in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    591\u001b[0m \u001b[39m@replace_list_option_in_docstrings\u001b[39m(TOKENIZER_MAPPING_NAMES)\n\u001b[0;32m    592\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_pretrained\u001b[39m(\u001b[39mcls\u001b[39m, pretrained_model_name_or_path, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    593\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    594\u001b[0m \u001b[39m    Instantiate one of the tokenizer classes of the library from a pretrained model vocabulary.\u001b[39;00m\n\u001b[0;32m    595\u001b[0m \n\u001b[0;32m    596\u001b[0m \u001b[39m    The tokenizer class to instantiate is selected based on the `model_type` property of the config object (either\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[39m    passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[39m    falling back to using pattern matching on `pretrained_model_name_or_path`:\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \n\u001b[0;32m    600\u001b[0m \u001b[39m    List options\u001b[39;00m\n\u001b[0;32m    601\u001b[0m \n\u001b[0;32m    602\u001b[0m \u001b[39m    Params:\u001b[39;00m\n\u001b[0;32m    603\u001b[0m \u001b[39m        pretrained_model_name_or_path (`str` or `os.PathLike`):\u001b[39;00m\n\u001b[0;32m    604\u001b[0m \u001b[39m            Can be either:\u001b[39;00m\n\u001b[0;32m    605\u001b[0m \n\u001b[0;32m    606\u001b[0m \u001b[39m                - A string, the *model id* of a predefined tokenizer hosted inside a model repo on huggingface.co.\u001b[39;00m\n\u001b[0;32m    607\u001b[0m \u001b[39m                  Valid model ids can be located at the root-level, like `bert-base-uncased`, or namespaced under a\u001b[39;00m\n\u001b[0;32m    608\u001b[0m \u001b[39m                  user or organization name, like `dbmdz/bert-base-german-cased`.\u001b[39;00m\n\u001b[0;32m    609\u001b[0m \u001b[39m                - A path to a *directory* containing vocabulary files required by the tokenizer, for instance saved\u001b[39;00m\n\u001b[0;32m    610\u001b[0m \u001b[39m                  using the [`~PreTrainedTokenizer.save_pretrained`] method, e.g., `./my_model_directory/`.\u001b[39;00m\n\u001b[0;32m    611\u001b[0m \u001b[39m                - A path or url to a single saved vocabulary file if and only if the tokenizer only requires a\u001b[39;00m\n\u001b[0;32m    612\u001b[0m \u001b[39m                  single vocabulary file (like Bert or XLNet), e.g.: `./my_model_directory/vocab.txt`. (Not\u001b[39;00m\n\u001b[0;32m    613\u001b[0m \u001b[39m                  applicable to all derived classes)\u001b[39;00m\n\u001b[0;32m    614\u001b[0m \u001b[39m        inputs (additional positional arguments, *optional*):\u001b[39;00m\n\u001b[0;32m    615\u001b[0m \u001b[39m            Will be passed along to the Tokenizer `__init__()` method.\u001b[39;00m\n\u001b[0;32m    616\u001b[0m \u001b[39m        config ([`PretrainedConfig`], *optional*)\u001b[39;00m\n\u001b[0;32m    617\u001b[0m \u001b[39m            The configuration object used to determine the tokenizer class to instantiate.\u001b[39;00m\n\u001b[0;32m    618\u001b[0m \u001b[39m        cache_dir (`str` or `os.PathLike`, *optional*):\u001b[39;00m\n\u001b[0;32m    619\u001b[0m \u001b[39m            Path to a directory in which a downloaded pretrained model configuration should be cached if the\u001b[39;00m\n\u001b[0;32m    620\u001b[0m \u001b[39m            standard cache should not be used.\u001b[39;00m\n\u001b[0;32m    621\u001b[0m \u001b[39m        force_download (`bool`, *optional*, defaults to `False`):\u001b[39;00m\n\u001b[0;32m    622\u001b[0m \u001b[39m            Whether or not to force the (re-)download the model weights and configuration files and override the\u001b[39;00m\n\u001b[0;32m    623\u001b[0m \u001b[39m            cached versions if they exist.\u001b[39;00m\n\u001b[0;32m    624\u001b[0m \u001b[39m        resume_download (`bool`, *optional*, defaults to `False`):\u001b[39;00m\n\u001b[0;32m    625\u001b[0m \u001b[39m            Whether or not to delete incompletely received files. Will attempt to resume the download if such a\u001b[39;00m\n\u001b[0;32m    626\u001b[0m \u001b[39m            file exists.\u001b[39;00m\n\u001b[0;32m    627\u001b[0m \u001b[39m        proxies (`Dict[str, str]`, *optional*):\u001b[39;00m\n\u001b[0;32m    628\u001b[0m \u001b[39m            A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\u001b[39;00m\n\u001b[0;32m    629\u001b[0m \u001b[39m            'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\u001b[39;00m\n\u001b[0;32m    630\u001b[0m \u001b[39m        revision (`str`, *optional*, defaults to `\"main\"`):\u001b[39;00m\n\u001b[0;32m    631\u001b[0m \u001b[39m            The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\u001b[39;00m\n\u001b[0;32m    632\u001b[0m \u001b[39m            git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\u001b[39;00m\n\u001b[0;32m    633\u001b[0m \u001b[39m            identifier allowed by git.\u001b[39;00m\n\u001b[0;32m    634\u001b[0m \u001b[39m        subfolder (`str`, *optional*):\u001b[39;00m\n\u001b[0;32m    635\u001b[0m \u001b[39m            In case the relevant files are located inside a subfolder of the model repo on huggingface.co (e.g. for\u001b[39;00m\n\u001b[0;32m    636\u001b[0m \u001b[39m            facebook/rag-token-base), specify it here.\u001b[39;00m\n\u001b[0;32m    637\u001b[0m \u001b[39m        use_fast (`bool`, *optional*, defaults to `True`):\u001b[39;00m\n\u001b[0;32m    638\u001b[0m \u001b[39m            Use a [fast Rust-based tokenizer](https://huggingface.co/docs/tokenizers/index) if it is supported for\u001b[39;00m\n\u001b[0;32m    639\u001b[0m \u001b[39m            a given model. If a fast tokenizer is not available for a given model, a normal Python-based tokenizer\u001b[39;00m\n\u001b[0;32m    640\u001b[0m \u001b[39m            is returned instead.\u001b[39;00m\n\u001b[0;32m    641\u001b[0m \u001b[39m        tokenizer_type (`str`, *optional*):\u001b[39;00m\n\u001b[0;32m    642\u001b[0m \u001b[39m            Tokenizer type to be loaded.\u001b[39;00m\n\u001b[1;32m--> 643\u001b[0m \u001b[39m        trust_remote_code (`bool`, *optional*, defaults to `False`):\u001b[39;00m\n\u001b[0;32m    644\u001b[0m \u001b[39m            Whether or not to allow for custom models defined on the Hub in their own modeling files. This option\u001b[39;00m\n\u001b[0;32m    645\u001b[0m \u001b[39m            should only be set to `True` for repositories you trust and in which you have read the code, as it will\u001b[39;00m\n\u001b[0;32m    646\u001b[0m \u001b[39m            execute code present on the Hub on your local machine.\u001b[39;00m\n\u001b[0;32m    647\u001b[0m \u001b[39m        kwargs (additional keyword arguments, *optional*):\u001b[39;00m\n\u001b[0;32m    648\u001b[0m \u001b[39m            Will be passed to the Tokenizer `__init__()` method. Can be used to set special tokens like\u001b[39;00m\n\u001b[0;32m    649\u001b[0m \u001b[39m            `bos_token`, `eos_token`, `unk_token`, `sep_token`, `pad_token`, `cls_token`, `mask_token`,\u001b[39;00m\n\u001b[0;32m    650\u001b[0m \u001b[39m            `additional_special_tokens`. See parameters in the `__init__()` for more details.\u001b[39;00m\n\u001b[0;32m    651\u001b[0m \n\u001b[0;32m    652\u001b[0m \u001b[39m    Examples:\u001b[39;00m\n\u001b[0;32m    653\u001b[0m \n\u001b[0;32m    654\u001b[0m \u001b[39m    ```python\u001b[39;00m\n\u001b[0;32m    655\u001b[0m \u001b[39m    >>> from transformers import AutoTokenizer\u001b[39;00m\n\u001b[0;32m    656\u001b[0m \n\u001b[0;32m    657\u001b[0m \u001b[39m    >>> # Download vocabulary from huggingface.co and cache.\u001b[39;00m\n\u001b[0;32m    658\u001b[0m \u001b[39m    >>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\u001b[39;00m\n\u001b[0;32m    659\u001b[0m \n\u001b[0;32m    660\u001b[0m \u001b[39m    >>> # Download vocabulary from huggingface.co (user-uploaded) and cache.\u001b[39;00m\n\u001b[0;32m    661\u001b[0m \u001b[39m    >>> tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-german-cased\")\u001b[39;00m\n\u001b[0;32m    662\u001b[0m \n\u001b[0;32m    663\u001b[0m \u001b[39m    >>> # If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained('./test/saved_model/')*)\u001b[39;00m\n\u001b[0;32m    664\u001b[0m \u001b[39m    >>> # tokenizer = AutoTokenizer.from_pretrained(\"./test/bert_saved_model/\")\u001b[39;00m\n\u001b[0;32m    665\u001b[0m \n\u001b[0;32m    666\u001b[0m \u001b[39m    >>> # Download vocabulary from huggingface.co and define model-specific arguments\u001b[39;00m\n\u001b[0;32m    667\u001b[0m \u001b[39m    >>> tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\", add_prefix_space=True)\u001b[39;00m\n\u001b[0;32m    668\u001b[0m \u001b[39m    ```\"\"\"\u001b[39;00m\n\u001b[0;32m    669\u001b[0m     use_auth_token \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39muse_auth_token\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    670\u001b[0m     \u001b[39mif\u001b[39;00m use_auth_token \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\GJAISCHOOL\\.conda\\envs\\nv38\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:487\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[1;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_tokenizer_config\u001b[39m(\n\u001b[0;32m    467\u001b[0m     pretrained_model_name_or_path: Union[\u001b[39mstr\u001b[39m, os\u001b[39m.\u001b[39mPathLike],\n\u001b[0;32m    468\u001b[0m     cache_dir: Optional[Union[\u001b[39mstr\u001b[39m, os\u001b[39m.\u001b[39mPathLike]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    476\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    477\u001b[0m ):\n\u001b[0;32m    478\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    479\u001b[0m \u001b[39m    Loads the tokenizer configuration from a pretrained model tokenizer configuration.\u001b[39;00m\n\u001b[0;32m    480\u001b[0m \n\u001b[0;32m    481\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    482\u001b[0m \u001b[39m        pretrained_model_name_or_path (`str` or `os.PathLike`):\u001b[39;00m\n\u001b[0;32m    483\u001b[0m \u001b[39m            This can be either:\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \n\u001b[0;32m    485\u001b[0m \u001b[39m            - a string, the *model id* of a pretrained model configuration hosted inside a model repo on\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[39m              huggingface.co. Valid model ids can be located at the root-level, like `bert-base-uncased`, or namespaced\u001b[39;00m\n\u001b[1;32m--> 487\u001b[0m \u001b[39m              under a user or organization name, like `dbmdz/bert-base-german-cased`.\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[39m            - a path to a *directory* containing a configuration file saved using the\u001b[39;00m\n\u001b[0;32m    489\u001b[0m \u001b[39m              [`~PreTrainedTokenizer.save_pretrained`] method, e.g., `./my_model_directory/`.\u001b[39;00m\n\u001b[0;32m    490\u001b[0m \n\u001b[0;32m    491\u001b[0m \u001b[39m        cache_dir (`str` or `os.PathLike`, *optional*):\u001b[39;00m\n\u001b[0;32m    492\u001b[0m \u001b[39m            Path to a directory in which a downloaded pretrained model configuration should be cached if the standard\u001b[39;00m\n\u001b[0;32m    493\u001b[0m \u001b[39m            cache should not be used.\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \u001b[39m        force_download (`bool`, *optional*, defaults to `False`):\u001b[39;00m\n\u001b[0;32m    495\u001b[0m \u001b[39m            Whether or not to force to (re-)download the configuration files and override the cached versions if they\u001b[39;00m\n\u001b[0;32m    496\u001b[0m \u001b[39m            exist.\u001b[39;00m\n\u001b[0;32m    497\u001b[0m \u001b[39m        resume_download (`bool`, *optional*, defaults to `False`):\u001b[39;00m\n\u001b[0;32m    498\u001b[0m \u001b[39m            Whether or not to delete incompletely received file. Attempts to resume the download if such a file exists.\u001b[39;00m\n\u001b[0;32m    499\u001b[0m \u001b[39m        proxies (`Dict[str, str]`, *optional*):\u001b[39;00m\n\u001b[0;32m    500\u001b[0m \u001b[39m            A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\u001b[39;00m\n\u001b[0;32m    501\u001b[0m \u001b[39m            'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.\u001b[39;00m\n\u001b[0;32m    502\u001b[0m \u001b[39m        token (`str` or *bool*, *optional*):\u001b[39;00m\n\u001b[0;32m    503\u001b[0m \u001b[39m            The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\u001b[39;00m\n\u001b[0;32m    504\u001b[0m \u001b[39m            when running `huggingface-cli login` (stored in `~/.huggingface`).\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[39m        revision (`str`, *optional*, defaults to `\"main\"`):\u001b[39;00m\n\u001b[0;32m    506\u001b[0m \u001b[39m            The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\u001b[39;00m\n\u001b[0;32m    507\u001b[0m \u001b[39m            git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\u001b[39;00m\n\u001b[0;32m    508\u001b[0m \u001b[39m            identifier allowed by git.\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \u001b[39m        local_files_only (`bool`, *optional*, defaults to `False`):\u001b[39;00m\n\u001b[0;32m    510\u001b[0m \u001b[39m            If `True`, will only try to load the tokenizer configuration from local files.\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[39m        subfolder (`str`, *optional*, defaults to `\"\"`):\u001b[39;00m\n\u001b[0;32m    512\u001b[0m \u001b[39m            In case the tokenizer config is located inside a subfolder of the model repo on huggingface.co, you can\u001b[39;00m\n\u001b[0;32m    513\u001b[0m \u001b[39m            specify the folder name here.\u001b[39;00m\n\u001b[0;32m    514\u001b[0m \n\u001b[0;32m    515\u001b[0m \u001b[39m    <Tip>\u001b[39;00m\n\u001b[0;32m    516\u001b[0m \n\u001b[0;32m    517\u001b[0m \u001b[39m    Passing `token=True` is required when you want to use a private model.\u001b[39;00m\n\u001b[0;32m    518\u001b[0m \n\u001b[0;32m    519\u001b[0m \u001b[39m    </Tip>\u001b[39;00m\n\u001b[0;32m    520\u001b[0m \n\u001b[0;32m    521\u001b[0m \u001b[39m    Returns:\u001b[39;00m\n\u001b[0;32m    522\u001b[0m \u001b[39m        `Dict`: The configuration of the tokenizer.\u001b[39;00m\n\u001b[0;32m    523\u001b[0m \n\u001b[0;32m    524\u001b[0m \u001b[39m    Examples:\u001b[39;00m\n\u001b[0;32m    525\u001b[0m \n\u001b[0;32m    526\u001b[0m \u001b[39m    ```python\u001b[39;00m\n\u001b[0;32m    527\u001b[0m \u001b[39m    # Download configuration from huggingface.co and cache.\u001b[39;00m\n\u001b[0;32m    528\u001b[0m \u001b[39m    tokenizer_config = get_tokenizer_config(\"bert-base-uncased\")\u001b[39;00m\n\u001b[0;32m    529\u001b[0m \u001b[39m    # This model does not have a tokenizer config so the result will be an empty dict.\u001b[39;00m\n\u001b[0;32m    530\u001b[0m \u001b[39m    tokenizer_config = get_tokenizer_config(\"xlm-roberta-base\")\u001b[39;00m\n\u001b[0;32m    531\u001b[0m \n\u001b[0;32m    532\u001b[0m \u001b[39m    # Save a pretrained tokenizer locally and you can reload its config\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \u001b[39m    from transformers import AutoTokenizer\u001b[39;00m\n\u001b[0;32m    534\u001b[0m \n\u001b[0;32m    535\u001b[0m \u001b[39m    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\u001b[39;00m\n\u001b[0;32m    536\u001b[0m \u001b[39m    tokenizer.save_pretrained(\"tokenizer-test\")\u001b[39;00m\n\u001b[0;32m    537\u001b[0m \u001b[39m    tokenizer_config = get_tokenizer_config(\"tokenizer-test\")\u001b[39;00m\n\u001b[0;32m    538\u001b[0m \u001b[39m    ```\"\"\"\u001b[39;00m\n\u001b[0;32m    539\u001b[0m     use_auth_token \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39muse_auth_token\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    540\u001b[0m     \u001b[39mif\u001b[39;00m use_auth_token \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\GJAISCHOOL\\.conda\\envs\\nv38\\lib\\site-packages\\transformers\\utils\\hub.py:417\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    412\u001b[0m     cache_dir \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(cache_dir)\n\u001b[0;32m    414\u001b[0m \u001b[39mif\u001b[39;00m _commit_hash \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m force_download:\n\u001b[0;32m    415\u001b[0m     \u001b[39m# If the file is cached under that commit hash, we return it directly.\u001b[39;00m\n\u001b[0;32m    416\u001b[0m     resolved_file \u001b[39m=\u001b[39m try_to_load_from_cache(\n\u001b[1;32m--> 417\u001b[0m         path_or_repo_id, full_filename, cache_dir\u001b[39m=\u001b[39mcache_dir, revision\u001b[39m=\u001b[39m_commit_hash, repo_type\u001b[39m=\u001b[39mrepo_type\n\u001b[0;32m    418\u001b[0m     )\n\u001b[0;32m    419\u001b[0m     \u001b[39mif\u001b[39;00m resolved_file \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    420\u001b[0m         \u001b[39mif\u001b[39;00m resolved_file \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m _CACHED_NO_EXIST:\n",
      "File \u001b[1;32mc:\\Users\\GJAISCHOOL\\.conda\\envs\\nv38\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\GJAISCHOOL\\.conda\\envs\\nv38\\lib\\site-packages\\huggingface_hub\\file_download.py:1217\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, endpoint, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[0;32m   1213\u001b[0m         \u001b[39mreturn\u001b[39;00m pointer_path\n\u001b[0;32m   1215\u001b[0m url \u001b[39m=\u001b[39m hf_hub_url(repo_id, filename, repo_type\u001b[39m=\u001b[39mrepo_type, revision\u001b[39m=\u001b[39mrevision, endpoint\u001b[39m=\u001b[39mendpoint)\n\u001b[1;32m-> 1217\u001b[0m headers \u001b[39m=\u001b[39m build_hf_headers(\n\u001b[0;32m   1218\u001b[0m     token\u001b[39m=\u001b[39;49mtoken,\n\u001b[0;32m   1219\u001b[0m     library_name\u001b[39m=\u001b[39;49mlibrary_name,\n\u001b[0;32m   1220\u001b[0m     library_version\u001b[39m=\u001b[39;49mlibrary_version,\n\u001b[0;32m   1221\u001b[0m     user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m   1222\u001b[0m )\n\u001b[0;32m   1224\u001b[0m url_to_download \u001b[39m=\u001b[39m url\n\u001b[0;32m   1225\u001b[0m etag \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\GJAISCHOOL\\.conda\\envs\\nv38\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\GJAISCHOOL\\.conda\\envs\\nv38\\lib\\site-packages\\huggingface_hub\\utils\\_headers.py:121\u001b[0m, in \u001b[0;36mbuild_hf_headers\u001b[1;34m(token, is_write_action, library_name, library_version, user_agent)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[39mBuild headers dictionary to send in a HF Hub call.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39m        If `token=True` but token is not saved locally.\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[39m# Get auth token to send\u001b[39;00m\n\u001b[1;32m--> 121\u001b[0m token_to_send \u001b[39m=\u001b[39m get_token_to_send(token)\n\u001b[0;32m    122\u001b[0m _validate_token_to_send(token_to_send, is_write_action\u001b[39m=\u001b[39mis_write_action)\n\u001b[0;32m    124\u001b[0m \u001b[39m# Combine headers\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\GJAISCHOOL\\.conda\\envs\\nv38\\lib\\site-packages\\huggingface_hub\\utils\\_headers.py:153\u001b[0m, in \u001b[0;36mget_token_to_send\u001b[1;34m(token)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mif\u001b[39;00m token \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    152\u001b[0m     \u001b[39mif\u001b[39;00m cached_token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 153\u001b[0m         \u001b[39mraise\u001b[39;00m LocalTokenNotFoundError(\n\u001b[0;32m    154\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mToken is required (`token=True`), but no token found. You\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    155\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m need to provide a token or be logged in to Hugging Face with\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    156\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m `huggingface-cli login` or `huggingface_hub.login`. See\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    157\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m https://huggingface.co/settings/tokens.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    158\u001b[0m         )\n\u001b[0;32m    159\u001b[0m     \u001b[39mreturn\u001b[39;00m cached_token\n\u001b[0;32m    161\u001b[0m \u001b[39m# Case implicit use of the token is forbidden by env variable\u001b[39;00m\n",
      "\u001b[1;31mLocalTokenNotFoundError\u001b[0m: Token is required (`token=True`), but no token found. You need to provide a token or be logged in to Hugging Face with `huggingface-cli login` or `huggingface_hub.login`. See https://huggingface.co/settings/tokens."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"hyunwoongko/kcbart-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, use_auth_token=True)\n",
    "\n",
    "input_sentence = \"한국어 문장 생성 테스트입니다.\"\n",
    "\n",
    "input_ids = tokenizer.encode(input_sentence, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "output = model.generate(input_ids, max_length=50, num_beams=5, length_penalty=2.0, early_stopping=True)\n",
    "\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"입력 문장:\", input_sentence)\n",
    "print(\"생성된 텍스트:\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$HUGGINGFACE_TOKEN\n"
     ]
    }
   ],
   "source": [
    "!echo $HUGGINGFACE_TOKEN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nv38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
