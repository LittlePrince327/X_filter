{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# beomi/KcELECTRA-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 2034573008267837899\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 82448744448\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 8707885697732648248\n",
      "physical_device_desc: \"device: 0, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:00:05.0, compute capability: 9.0\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# %run KcELECTRA-base.version.py     # 필요한 라이브러리 설치\n",
    "# !pip freeze                          # 설치된 라이브러리 확인\n",
    "# Learning Device list Check \n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# GPU 설정\n",
    "# 없을 시 CPU\n",
    "# CPU로 뜨지만, GPU 잘만 돌아감\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\GJAISCHOOL\\\\Desktop\\\\X_filter\\\\Algorithm\\\\dataset\\\\sample_data(500).xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\administrator\\Desktop\\X_filter\\Algorithm\\beomi_KcELECTRA-base\\beomi_KcELECTRA-base.ipynb 셀 4\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/administrator/Desktop/X_filter/Algorithm/beomi_KcELECTRA-base/beomi_KcELECTRA-base.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# 엑셀 파일에서 데이터프레임 읽기\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/administrator/Desktop/X_filter/Algorithm/beomi_KcELECTRA-base/beomi_KcELECTRA-base.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_excel(\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC:\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mUsers\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mGJAISCHOOL\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mDesktop\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mX_filter\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mAlgorithm\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mdataset\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39msample_data(500).xlsx\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/administrator/Desktop/X_filter/Algorithm/beomi_KcELECTRA-base/beomi_KcELECTRA-base.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(df\u001b[39m.\u001b[39mshape)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/administrator/Desktop/X_filter/Algorithm/beomi_KcELECTRA-base/beomi_KcELECTRA-base.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m df\u001b[39m.\u001b[39mhead()\n",
      "File \u001b[1;32mc:\\Users\\administrator\\anaconda3\\envs\\new_py\\lib\\site-packages\\pandas\\io\\excel\\_base.py:478\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(io, ExcelFile):\n\u001b[0;32m    477\u001b[0m     should_close \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 478\u001b[0m     io \u001b[39m=\u001b[39m ExcelFile(io, storage_options\u001b[39m=\u001b[39;49mstorage_options, engine\u001b[39m=\u001b[39;49mengine)\n\u001b[0;32m    479\u001b[0m \u001b[39melif\u001b[39;00m engine \u001b[39mand\u001b[39;00m engine \u001b[39m!=\u001b[39m io\u001b[39m.\u001b[39mengine:\n\u001b[0;32m    480\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    481\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mEngine should not be specified when passing \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    482\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    483\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\administrator\\anaconda3\\envs\\new_py\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1496\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[0;32m   1494\u001b[0m     ext \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxls\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1495\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1496\u001b[0m     ext \u001b[39m=\u001b[39m inspect_excel_format(\n\u001b[0;32m   1497\u001b[0m         content_or_path\u001b[39m=\u001b[39;49mpath_or_buffer, storage_options\u001b[39m=\u001b[39;49mstorage_options\n\u001b[0;32m   1498\u001b[0m     )\n\u001b[0;32m   1499\u001b[0m     \u001b[39mif\u001b[39;00m ext \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1500\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1501\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mExcel file format cannot be determined, you must specify \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1502\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39man engine manually.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1503\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\administrator\\anaconda3\\envs\\new_py\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1371\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1368\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(content_or_path, \u001b[39mbytes\u001b[39m):\n\u001b[0;32m   1369\u001b[0m     content_or_path \u001b[39m=\u001b[39m BytesIO(content_or_path)\n\u001b[1;32m-> 1371\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n\u001b[0;32m   1372\u001b[0m     content_or_path, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m, storage_options\u001b[39m=\u001b[39;49mstorage_options, is_text\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[0;32m   1373\u001b[0m ) \u001b[39mas\u001b[39;00m handle:\n\u001b[0;32m   1374\u001b[0m     stream \u001b[39m=\u001b[39m handle\u001b[39m.\u001b[39mhandle\n\u001b[0;32m   1375\u001b[0m     stream\u001b[39m.\u001b[39mseek(\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\administrator\\anaconda3\\envs\\new_py\\lib\\site-packages\\pandas\\io\\common.py:868\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    859\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\n\u001b[0;32m    860\u001b[0m             handle,\n\u001b[0;32m    861\u001b[0m             ioargs\u001b[39m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    864\u001b[0m             newline\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(handle, ioargs\u001b[39m.\u001b[39;49mmode)\n\u001b[0;32m    869\u001b[0m     handles\u001b[39m.\u001b[39mappend(handle)\n\u001b[0;32m    871\u001b[0m \u001b[39m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\GJAISCHOOL\\\\Desktop\\\\X_filter\\\\Algorithm\\\\dataset\\\\sample_data(500).xlsx'"
     ]
    }
   ],
   "source": [
    "# 엑셀 파일에서 데이터프레임 읽기\n",
    "df = pd.read_excel(r\"C:\\Users\\GJAISCHOOL\\Desktop\\X_filter\\Algorithm\\dataset\\kc_Sentence_data.xlsx\")\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_idx = df[df.label.isnull()].index                             # 해당 index에 null 값 확인\n",
    "df.loc[null_idx, \"Sentence\"]                                       # null 값이 존재한 인덱스의 content 값 불러오기\n",
    "\n",
    "# lable은 content의 가장 끝 문장열로 설정\n",
    "df.loc[null_idx, \"label\"] = df.loc[null_idx, \"Sentence\"].apply(lambda x: x[-1])\n",
    "\n",
    "# content는 \"\\t\" 앞부분까지의 문자열로 설정\n",
    "df.loc[null_idx, \"Sentence\"] = df.loc[null_idx, \"Sentence\"].apply(lambda x: x[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중복 제거 전 학습 데이터셋 : 407\n",
      "중복 제거 전 테스트 데이터셋 : 102\n",
      "중복 제거 후 학습 데이터셋 : 407\n",
      "중복 제거 후 테스트 데이터셋 : 102\n"
     ]
    }
   ],
   "source": [
    "train_data = df.sample(frac=0.8, random_state=42)                 # train(80%), test(20%) 셋 구분 \n",
    "test_data = df.drop(train_data.index)                             # 랜덤으로 샘플링(랜덤으로 숫자 배치)\n",
    "\n",
    "# 데이터셋 갯수 확인\n",
    "print('중복 제거 전 학습 데이터셋 : {}'.format(len(train_data)))\n",
    "print('중복 제거 전 테스트 데이터셋 : {}'.format(len(test_data)))\n",
    "\n",
    "# 중복 데이터 제거\n",
    "train_data.drop_duplicates(subset=[\"Sentence\"], inplace=True)\n",
    "test_data.drop_duplicates(subset=[\"Sentence\"], inplace=True)\n",
    "\n",
    "# 데이터셋 갯수 확인\n",
    "print('중복 제거 후 학습 데이터셋 : {}'.format(len(train_data)))\n",
    "print('중복 제거 후 테스트 데이터셋 : {}'.format(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\GJAISCHOOL\\.conda\\envs\\nv_decem\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import ElectraTokenizer, ElectraForSequenceClassification, TFElectraForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "\n",
    "# KcELECTRA 모델 및 토크나이저 로드\n",
    "Kc_model = \"beomi/KcELECTRA-base\"\n",
    "Kc_tokenizer = AutoTokenizer.from_pretrained(Kc_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>우리나라였으면 남성1년 집해유예, 엄마 20년 때릴것같네요. 아님말구...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>저 양반은 어쩌다 저리 됐을까요?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>물도 꼭 챙겨마셔라 안마시면 죽더라 진짜임</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>대중이 까는거 무적권 ㅇㅂ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>일단은 표현의 자유로 남겨놔야하는 것 같음</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>연분9등법은</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>지금의 정부는 다 계획이 있구나~</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>씨발년아 웃겨?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>ㅅㅂ 인구가 준다는데 수도권에 더 많이 살아. 좀 갈사람은 나가라. 특히 586세대들</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>경찰 vs 군무원 어떤게 더 낫냐</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>407 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  label\n",
       "79         우리나라였으면 남성1년 집해유예, 엄마 20년 때릴것같네요. 아님말구...      0\n",
       "317                               저 양반은 어쩌다 저리 됐을까요?      0\n",
       "486                          물도 꼭 챙겨마셔라 안마시면 죽더라 진짜임      0\n",
       "397                                   대중이 까는거 무적권 ㅇㅂ      0\n",
       "167                          일단은 표현의 자유로 남겨놔야하는 것 같음      0\n",
       "..                                               ...    ...\n",
       "442                                           연분9등법은      0\n",
       "438                               지금의 정부는 다 계획이 있구나~      0\n",
       "337                                         씨발년아 웃겨?      1\n",
       "236  ㅅㅂ 인구가 준다는데 수도권에 더 많이 살아. 좀 갈사람은 나가라. 특히 586세대들      1\n",
       "207                               경찰 vs 군무원 어떤게 더 낫냐      0\n",
       "\n",
       "[407 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_sentences = Kc_tokenizer( \n",
    "    list(train_data[\"Sentence\"]),           # train_data 리스트화\n",
    "    return_tensors=\"pt\",                    # pytorch의 tensor 형태로 변환\n",
    "    max_length=128,                         # 최대 토큰길이 설정\n",
    "    padding=True,                           # 학습 시 빈 값을 0으로 대체\n",
    "    truncation=True,                        # 초과 토큰 제거\n",
    "    add_special_tokens=True                 # 추가 토큰 설정\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test_sentences = Kc_tokenizer(\n",
    "    list(test_data[\"Sentence\"]),           # test_data 리스트화\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=128,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    add_special_tokens=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCLS(분류 토큰)   : 시작을 알릴 때 사용\\nSEP(구분자 토큰) : 서로 다를 때 구분할 사용할 토큰\\nMASK            : 특정 토큰이 마스크화, 모델이 예측하도록 훈련\\nPAD             : 일관된 길이로 동일하게 만듦\\n                                                            '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "CLS(분류 토큰)   : 시작을 알릴 때 사용\n",
    "SEP(구분자 토큰) : 서로 다를 때 구분할 사용할 토큰\n",
    "MASK            : 특정 토큰이 마스크화, 모델이 예측하도록 훈련\n",
    "PAD             : 일관된 길이로 동일하게 만듦\n",
    "                                                            \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding(num_tokens=110, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
      "========================================================================================================================================================================================================\n",
      "['Ġìļ°ë¦¬ëĤĺëĿ¼', 'ìĺĢìľ¼ë©´', 'ĠëĤ¨ìĦ±', '1', 'ëħĦ', 'Ġì§ĳ', 'íķ´', 'ìľł', 'ìĺĪ', ',', 'ĠìĹĦë§Ī', 'Ġ20', 'ëħĦ', 'ĠëķĮë¦´', 'ê²ĥê°ĻëĦ¤ìļĶ', '.', 'ĠìķĦëĭĺ', 'ë§Ĳ', 'êµ¬', '...', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "========================================================================================================================================================================================================\n",
      "[1158, 6167, 4875, 21, 604, 639, 305, 608, 1193, 16, 3291, 1321, 604, 27887, 28472, 18, 1852, 473, 480, 351, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_sentences[0])        # 0번째 문장에 해당하는 객체\n",
    "print(\"=\"*200) \n",
    "print(tokenized_train_sentences[0].tokens) # 0번째 문장에 토큰의 목록\n",
    "print(\"=\"*200)\n",
    "print(tokenized_train_sentences[0].ids)    # 0번째 문장에 대한 고유 ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CurseDataset(torch.utils.data.Dataset):                                        # 학습을 위한 데이터셋 만들기     \n",
    "    def __init__(self, encoding, labels):                                            # pytorch에서 학습할 수 있게 데이터셋 생성\n",
    "        self.encodings = encoding                                                    # Feature Data / 'Sentence'\n",
    "        self.labels = labels                                                         # Target Data / 'Label'\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()} # self.encodings 딕셔너리 내의 값 중에 val를 torch.tensor로 변환해 하여\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])                             # key: toch.tensor(val[ix])라는 item 딕셔너리 형성\n",
    "        return item                                                                 # 새로운 labels 키 값에 value torch.tensor(self.labels[idx]) 쌍을 생성\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)                                                     # self.labels 길이 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set, test_set에 대한 데이터셋을 각각 생성\n",
    "train_label = train_data[\"label\"].values\n",
    "test_label = test_data[\"label\"].values\n",
    "\n",
    "train_dataset = CurseDataset(tokenized_train_sentences, train_label)\n",
    "test_dataset = CurseDataset(tokenized_test_sentences, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at beomi/KcELECTRA-base were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at beomi/KcELECTRA-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ElectraForSequenceClassification(\n",
       "  (electra): ElectraModel(\n",
       "    (embeddings): ElectraEmbeddings(\n",
       "      (word_embeddings): Embedding(30000, 768, padding_idx=3)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): ElectraEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): ElectraClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Kc_model = AutoModelForSequenceClassification.from_pretrained(Kc_model, num_labels=2)   # 사전 학습된 모델 찾아오기\n",
    "Kc_model.to(device)                                                                     # num_labels 평가지표 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - 2 학습 파라미터 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = './results',           # 학습결과 저장경로\n",
    "    num_train_epochs=10,                # 학습 epoch 설정\n",
    "    per_device_train_batch_size = 8,    # train batch_size 설정\n",
    "    per_device_eval_batch_size = 64,    # test batch_size 설정\n",
    "    logging_dir = './logs',             # 학습 log 저장경로 / 손실 값 저장\n",
    "    logging_steps=500,                  # 학습 log 기록 단위 / 500번 마다 출력\n",
    "    save_total_limit = 2,               # 학습결과 저장 최대갯수\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "# 학습과정에서 사용할 평가지표를 위한 함수 설정\n",
    "def compute_metrics(pred):\n",
    "  labels = pred.label_ids\n",
    "  preds = pred.predictions.argmax(-1)\n",
    "  # 정밀도, 재현율, f1 구하기 \n",
    "  precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "  # 정확도 구하기\n",
    "  acc = accuracy_score(labels, preds)\n",
    "  return{\n",
    "      'accuracy': acc,\n",
    "      'f1': f1,\n",
    "      'precision': precision,\n",
    "      'recall': recall\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer 모듈을 사용해 모델의 학습을 컨트롤하은 trainer를 생성\n",
    "trainer = Trainer(\n",
    "    model = Kc_model,                    # 학습하고자하는 Transformers model\n",
    "    args=training_args,                  # 위에서 정의한 학습 피라미터 설정\n",
    "    train_dataset=train_dataset,         # 학습 데이터셋\n",
    "    eval_dataset=test_dataset,           # 평가 데이터셋\n",
    "    compute_metrics=compute_metrics,     # 평가지표\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.trainer.Trainer'>\n"
     ]
    }
   ],
   "source": [
    "print(type(trainer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\GJAISCHOOL\\.conda\\envs\\nv_decem\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/510 [00:00<?, ?it/s]C:\\Users\\GJAISCHOOL\\AppData\\Local\\Temp\\ipykernel_4536\\1869798269.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()} # self.encodings 딕셔너리 내의 값 중에 val를 torch.tensor로 변환해 하여\n",
      " 18%|█▊        | 91/510 [06:23<25:54,  3.71s/it]"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Evaluate on the test dataset\n",
    "results = trainer.predict(test_dataset)\n",
    "\n",
    "# Get the true labels and predicted logits\n",
    "true_labels = results.label_ids\n",
    "predicted_logits = results.predictions.argmax(-1)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_mat = confusion_matrix(true_labels, predicted_logits)\n",
    "\n",
    "# Plot the confusion matrix using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=['P', 'N'], yticklabels=['T', 'F'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_predict(sent):\n",
    "    Kc_model.eval()\n",
    "\n",
    "    Kc_tokenized_sent = Kc_tokenizer(\n",
    "        text_target=sent,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "    Kc_tokenized_sent.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = Kc_model(\n",
    "        input_ids=Kc_tokenized_sent[\"input_ids\"],\n",
    "        attention_mask=Kc_tokenized_sent[\"attention_mask\"],\n",
    "        token_type_ids=Kc_tokenized_sent[\"token_type_ids\"]\n",
    "    )\n",
    "\n",
    "    logits = outputs[0]\n",
    "    logits = logits.detach().cpu()\n",
    "    result = logits.argmax(-1)\n",
    "    if result == 0:\n",
    "        return False, sent\n",
    "    elif result == 1:\n",
    "        return True, sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentence_predict(\"화이팅\"))\n",
    "print(sentence_predict(\"이게 맞아?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'RNN_2' 모듈을 가져오기 위한 경로를 sys.path에 추가\n",
    "import sys\n",
    "sys.path.append(r'C:\\Users\\administrator\\Desktop\\X_filter\\Algorithm\\RNN')\n",
    " \n",
    "# 'RNN' 모듈을 import\n",
    "from RNN import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def badword_find(sent):\n",
    "    badword_df = pd.read_excel(r'C:\\Users\\GJAISCHOOL\\Desktop\\X_filter\\Algorithm\\dataset\\word_list (완성).xlsx')\n",
    "    found_bad_word = False  # 입력 문장에 단어가 발견되었는지를 나타내는 플래그\n",
    "    for idx, row in badword_df.iterrows():\n",
    "        if row[\"WORD\"] in sent:\n",
    "            # 'WORD'가 입력 문장에 포함된 경우\n",
    "            new_word = row[\"대체어\"]\n",
    "            if not pd.isnull(new_word):\n",
    "                # RNN_result = sentence_generation(new_word, 2) + \" \"\n",
    "                sent = sent.replace(row[\"WORD\"], new_word)\n",
    "                found_bad_word = True\n",
    "            else:\n",
    "                sent = sent.replace(row[\"WORD\"], \"*\" * len(row[\"WORD\"]))\n",
    "                found_bad_word = True\n",
    "    \n",
    "    if not found_bad_word:\n",
    "        # result = \"@\" * len(input_sentence)\n",
    "        sent = \"혐오 표현입니다.\"\n",
    "    return sent\n",
    "\n",
    "# 테스트\n",
    "input_sentence = \"씨발\"\n",
    "speak_result = badword_find(input_sentence)\n",
    "print(speak_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speak_pre(sent):\n",
    "    is_malicious, _ = sentence_predict(sent)\n",
    "    if not is_malicious:\n",
    "        return sent\n",
    "    else:\n",
    "        return badword_find(sent)\n",
    "\n",
    "def speak(sent):\n",
    "    is_malicious, sent_result = sentence_predict(sent)\n",
    "    if not is_malicious:\n",
    "        return sent\n",
    "    else:\n",
    "        return badword_find(sent_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import kss\n",
    "from pykospacing import Spacing\n",
    "from soynlp.normalizer import repeat_normalize\n",
    "\n",
    "# 띄어쓰기 설정\n",
    "spacing = Spacing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특수문자 제거\n",
    "def cleanse(text):\n",
    "    pattern = re.compile(r'\\s+')\n",
    "    text = re.sub(pattern, ' ', text)\n",
    "    text = re.sub('[^가-힣ㄱ-ㅎㅏ|a-zA-Z0-9]','', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_repeat_normalize(text):\n",
    "    cleansed_text = cleanse(text)                                             # 특수문자 제거\n",
    "    normalized_text = repeat_normalize(cleansed_text, num_repeats=2)          # 중복문자 제거\n",
    "    input_data = re.sub(r'\\d', '', normalized_text)                           # 제거\n",
    "    normalized_text = spacing(input_data)                                     # 띄어쓰기 보정 \n",
    "\n",
    "    return normalized_text\n",
    "\n",
    "clean_and_repeat_normalize(\"아버지가방에들어가신다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_output():\n",
    "    input_text = clean_and_repeat_normalize(input(\"텍스트를 입력하세요: \"))\n",
    "    sentences = kss.split_sentences(input_text)\n",
    "\n",
    "    sentences_list = []\n",
    "    for sentence in sentences:\n",
    "        result = speak(sentence)\n",
    "        sentences_list.append(result) \n",
    "\n",
    "    long_test = ' '.join(sentences_list)\n",
    "    return long_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시발 정작 지네들 자식은 몸쓰는일 안시킬거면서 개지랄떠네 ㅋㅋㅋ\n",
    "final_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "result_1 = subprocess.run(['pip', 'install', 'tensorflow'], capture_output=True, text=True)   # 필요 라이브러리 설치\n",
    "result_2 = subprocess.run(['pip', 'show', 'tensorflow'], capture_output=True, text=True)\n",
    "print(result_1.stdout)\n",
    "print(result_2.stdout)                                               \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\GJAISCHOOL\\Desktop\\sentence pl data1113 수정.csv')\n",
    "\n",
    "sentences = df['Sentence'].tolist()\n",
    "rnn_tokenizer = Tokenizer()\n",
    "rnn_tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "vocab_size = len(rnn_tokenizer.word_index) + 1                          # 어휘 사전에 크기 계산 \n",
    "\n",
    "sequences = list()                                                      # 각 문장을 단어 시퀀스로 변환하고, 문장의 부분 시퀀스를 생성하여 리스트에 추가\n",
    "for line in sentences:\n",
    "    encoded = rnn_tokenizer.texts_to_sequences([line])[0]               # 현재 문장을 정수로 시퀀스로 변수\n",
    "    for i in range(1, len(encoded)):                                    # 문장의 부분 시퀀스를 생성하고 리스트에 추가\n",
    "        sequence = encoded[: i+1]\n",
    "        sequences.append(sequence)\n",
    "\n",
    "max_len = max(len(l) for l in sequences)                                # 모든 샘플에서 길이가 가장 긴 샘플의 길이 출력\n",
    "sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')     # 시퀀스 데이터를 패딩하여 일정한 길이로 만듦 (여기서는 가장 긴 샘플의 길이로 패딩)\n",
    "\n",
    "sequences = np.array(sequences)                                         # 패딩이 된 시퀀스를 넘파이 배열로 변환\n",
    "                                                                        # 입력 및 출력 데이터 생성\n",
    "X = sequences[:, :-1]                                                   # 시퀀스에서 마지막 단어를 제외한 부분은 입력(X)\n",
    "y = sequences[:, -1]                                                    # 시퀀스에 마지막 단어는 출력\n",
    "y = to_categorical(y, num_classes=vocab_size)                           # 출력(y)을 one-hot 인코딩 형태로 변환\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, SimpleRNN\n",
    "\n",
    "embedding_dim = 10                                                      # 임베딩 차원을 10으로 10차원의 실수 벡터로 표현\n",
    "hidden_units = 32                                                       # 순환 신경망 내 내부에서의 뉴련의 수\n",
    "\n",
    "RNN_model = Sequential()                                                    \n",
    "RNN_model.add(Embedding(vocab_size, embedding_dim))                     # Embedding 레이어 추가: 어휘 사전의 크기를 입력으로 받고, 지정된 임베딩 차원으로 단어를 임베딩\n",
    "RNN_model.add(SimpleRNN(hidden_units))                                  # 지정된 은닉 유닛 수를 가진 단순한 RNN 레이어\n",
    "RNN_model.add(Dense(vocab_size, activation='softmax'))                  # 출력 레이어 추가 : 어휘 사전의 크길르 가진 Dense 레이어를 사용하며, 활성화 함수는 softmax\n",
    "RNN_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "                                                                        # 모델 컴파일: 다중 클래스 분류 문제이므로 categorical_crossentropy 손실 함수와 Adam 옵티마이저 사용\n",
    "RNN_model.fit(X, y, epochs=20, verbose=2)                               # 모델 학습: 입력(X) 및 출력(y) 데이터를 사용하여 주어진 에폭 동안 모델을 학습\n",
    "\n",
    "def sentence_generation(current_word, n, temperature=1.0, model=RNN_model, tokenizer=rnn_tokenizer):\n",
    "    init_word = current_word                                            # 시작 단어와 문장을 저장할 빈 문자열 초기화\n",
    "    sentence = ''\n",
    "\n",
    "    for _ in range(n):                                                  # n개 단어 만큼 문자 생성\n",
    "        encoded = rnn_tokenizer.texts_to_sequences([current_word])[0]   # 현재 단어를 토크나이저를 사용하여 정수 시퀀스로 변환 \n",
    "                                                                        # ex) \"Hello, how are you?\" => \"Hello\"가 1, \"how\"가 2, \"are\"가 3, \"you\"가 4라고 할 때 => [1, 2, 3, 4]\n",
    "        encoded = pad_sequences([encoded], maxlen=5, padding='pre')     # 시퀀스가 5로 설정하고, 부족하면 앞쪽에 0을 채워넣고 부족하면 앞부분을 자른다.\n",
    "                                                                        # 만약 뒤쪽으로 바꾸고싶으면 padding='post를 주면 된다.\n",
    "        result = RNN_model.predict(encoded, verbose=0)                  # 모델의 다음단어 예측\n",
    "        result = result / temperature                                   # 확률에 온도를 적용하여 랜덤성 조절\n",
    "        result = np.exp(result) / np.sum(np.exp(result), axis=1)        # 소프트맥스를 적용하여 최종 확률 얻기\n",
    "        sampled_index = np.random.choice(len(result[0]), p=result[0])   # 예측된 확률을 기반으로 다음 단어 샘플링\n",
    "        word = rnn_tokenizer.index_word[sampled_index]\n",
    "\n",
    "        current_word = current_word + ' ' + word                        # 현재 단어와 문장 업데이트\n",
    "        sentence = sentence + ' ' + word\n",
    "\n",
    "    sentence = init_word + sentence\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nv37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
