{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 13265457253283293606\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 83128483840\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 10020708011227127968\n",
      "physical_device_desc: \"device: 0, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:00:05.0, compute capability: 9.0\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import subprocess\n",
    "# # 파이썬 환경 구축 후 라이브러리 버전 맞춰 설치\n",
    "# packages_to_install = [\n",
    "#     \"transformers==4.30.2\",\n",
    "#     \"torch==2.1.0\",\n",
    "#     \"soynlp==0.0.493\",\n",
    "#     \"requests==2.31.0\",\n",
    "#     \"tensorflow==2.10.0\",\n",
    "#     \"accelerate==0.20.1\",\n",
    "#     \"kss==4.5.4\",\n",
    "#     \"matplotlib==3.7.3\",\n",
    "#     \"wordcloud==1.9.2\",\n",
    "#     \"JPype1==1.4.1\",\n",
    "#     \"rhinoMorph==4.0.1.12\",\n",
    "#     \"kiwipiepy==0.16.1\",\n",
    "#     \"Konlpy==0.6.0\",\n",
    "#     \"nltk==3.8.1\",\n",
    "#     \"seaborn==0.13.0\"\n",
    "# ]\n",
    "\n",
    "# for package in packages_to_install:\n",
    "#     os.system(f\"pip install {package}\")\n",
    "\n",
    "# # pip 업그레이드\n",
    "# os.system(\"python -m pip install --upgrade pip\")\n",
    "\n",
    "# # 파이썬 라이브러리 버전 전체 확인\n",
    "# os.system(\"pip freeze\")\n",
    "\n",
    "# # 파이썬 라이브러리 버전 하나씩 확인\n",
    "# packages_to_show = [\n",
    "#     \"transformers\",\n",
    "#     \"torch\",\n",
    "#     \"soynlp\",\n",
    "#     \"requests\",\n",
    "#     \"tensorflow\",\n",
    "#     \"accelerate\",\n",
    "#     \"kss\",\n",
    "#     \"matplotlib\",\n",
    "#     \"wordcloud\",\n",
    "#     \"JPype1\",\n",
    "#     \"rhinoMorph\",\n",
    "#     \"kiwipiepy\",\n",
    "#     \"Konlpy\",\n",
    "#     \"nltk\",\n",
    "#     \"seaborn\"\n",
    "# ]\n",
    "\n",
    "# for package in packages_to_show:\n",
    "#     os.system(f\"pip show {package}\")\n",
    "\n",
    "\n",
    "# # git에서 라이브러리 복제\n",
    "# os.system(\"git clone https://github.com/ZIZUN/korean-malicious-comments-dataset.git\")\n",
    "# os.system(\"bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\")\n",
    "\n",
    "# subprocess.call(\"pip install git+https://github.com/haven-jeon/PyKoSpacing.git\", shell=True)\n",
    "# print(\"라이브러리 설치가 완료되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda:0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "device=torch.device('cuda:0'if torch.cuda.is_available()else 'cpu')\n",
    "print(\"device =\",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>집에 롱 패딩만 세 개다. 10년 더 입어야지 ㅋㅋ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>세탁이라고 봐도 된다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>은행에 대출 상담 받으러 가보면 직업의 귀천 바로 알려줌</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ㅋㅋㅋㅋㅋ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ㄹㅇㅋㅋㅋ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Sentence  label\n",
       "0     집에 롱 패딩만 세 개다. 10년 더 입어야지 ㅋㅋ      0\n",
       "1                      세탁이라고 봐도 된다      0\n",
       "2  은행에 대출 상담 받으러 가보면 직업의 귀천 바로 알려줌      0\n",
       "3                            ㅋㅋㅋㅋㅋ      0\n",
       "4                            ㄹㅇㅋㅋㅋ      0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(r\"C:\\Users\\administrator\\Desktop\\X_filter\\AI_Model\\dataset\\sentence_data(cpp).xlsx\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_idx = df[df.label.isnull()].index                             # 해당 index에 null 값 확인\n",
    "df.loc[null_idx, \"Sentence\"]                                       # null 값이 존재한 인덱스의 content 값 불러오기\n",
    "\n",
    "# lable은 content의 가장 끝 문장열로 설정\n",
    "df.loc[null_idx, \"label\"] = df.loc[null_idx, \"Sentence\"].apply(lambda x: x[-1])\n",
    "\n",
    "# content는 \"\\t\" 앞부분까지의 문자열로 설정\n",
    "df.loc[null_idx, \"Sentence\"] = df.loc[null_idx, \"Sentence\"].apply(lambda x: x[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../Morphological_Analyzer/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중복 제거 전 학습 데이터셋 : 87998\n",
      "중복 제거 전 테스트 데이터셋 : 22000\n",
      "중복 제거 후 학습 데이터셋 : 84546\n",
      "중복 제거 후 테스트 데이터셋 : 21774\n"
     ]
    }
   ],
   "source": [
    "# from kiwipiepy import Kiwi\n",
    "from kiwi import *\n",
    "\n",
    "kiwi_pre_train = df.sample(frac=0.8, random_state=42)                     # train(80%), test(20%) 셋 구분 \n",
    "kiwi_pre_test = df.drop(kiwi_pre_train.index)                             # 랜덤으로 샘플링(랜덤으로 숫자 배치)\n",
    "\n",
    "# 데이터셋 갯수 확인\n",
    "print('중복 제거 전 학습 데이터셋 : {}'.format(len(kiwi_pre_train)))\n",
    "print('중복 제거 전 테스트 데이터셋 : {}'.format(len(kiwi_pre_test)))\n",
    "\n",
    "# 중복 데이터 제거\n",
    "kiwi_pre_train.drop_duplicates(subset=[\"Sentence\"], inplace=True)\n",
    "kiwi_pre_test.drop_duplicates(subset=[\"Sentence\"], inplace=True)\n",
    "\n",
    "# 형태소 분석 적용\n",
    "train_morp = list(kiwi.tokenize(kiwi_pre_train['Sentence'].tolist(), normalize_coda=True))\n",
    "test_morp = list(kiwi.tokenize(kiwi_pre_test['Sentence'].tolist(), normalize_coda=True))\n",
    "\n",
    "train_data = [train_morp, kiwi_pre_train['label']]\n",
    "test_data = [test_morp, kiwi_pre_test['label']]\n",
    "\n",
    "\n",
    "# 데이터셋 갯수 확인\n",
    "print('중복 제거 후 학습 데이터셋 : {}'.format(len(train_data[0])))\n",
    "print('중복 제거 후 테스트 데이터셋 : {}'.format(len(test_data[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 문장에서 최대 토큰 수 계산\n",
    "max_tokens = max(len(sentence) for sentence in train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0    1    2    3     4    5   6       7     8    9    ... 639 640 641  \\\n",
      "0        음   그렇   ᆫ가    ?                                    ...               \n",
      "1      강원도    는   꼴랑  700     억    주   고  ㅋㅋㅋㅋㅋㅋ   차이나   타운  ...               \n",
      "2        '    '  빨갱이    '     '    '   들      이제     ᆫ  대놓고  ...               \n",
      "3       그런   대사    뭐    이     지    ,   ?      소설     같    네  ...               \n",
      "4        '    '   미치    ᆫ     '    '   '       .  그야말로   사람  ...               \n",
      "...    ...  ...  ...  ...   ...  ...  ..     ...   ...  ...  ...  ..  ..  ..   \n",
      "84541   키키   같이   입사   도전     ?                              ...               \n",
      "84542   애초    에  유재석    은  우리나라  개돼지  국민       서     으    로  ...               \n",
      "84543  대가리    텅    비    ᆫ     년    들  ..                     ...               \n",
      "84544    불    에    타    는     줄    알   았       음   ...    덥  ...               \n",
      "84545  아니요  ...   아무    도     없   네요                         ...               \n",
      "\n",
      "      642 643 644 645 646 647 648  \n",
      "0                               0  \n",
      "1                               1  \n",
      "2                               1  \n",
      "3                               0  \n",
      "4                               1  \n",
      "...    ..  ..  ..  ..  ..  ..  ..  \n",
      "84541                           0  \n",
      "84542                           1  \n",
      "84543                           1  \n",
      "84544                           0  \n",
      "84545                           0  \n",
      "\n",
      "[84546 rows x 649 columns]\n"
     ]
    }
   ],
   "source": [
    "# DataFrame 생성\n",
    "train_df = pd.DataFrame(index=range(len(train_data[0])), columns=range(max_tokens + 1))\n",
    "\n",
    "# 각 문장에 대해 반복하면서 DataFrame 생성\n",
    "for i, sentence in enumerate(train_data[0]):\n",
    "    # DataFrame에 토큰 형태 추가\n",
    "    for j, token in enumerate(sentence):\n",
    "        train_df.loc[i, j] = token.form\n",
    "\n",
    "    # 마지막 열에 'label' 값 추가\n",
    "    train_df.loc[i, max_tokens] = train_data[1].iloc[i]\n",
    "\n",
    "# NaN 값을 빈 문자열로 대체\n",
    "train_df = train_df.fillna('')\n",
    "\n",
    "# 결과적인 DataFrame 출력\n",
    "print(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0    1   2    3    4   5    6    7   8    9    ... 639 640 641 642  \\\n",
      "0        은행    에  대출   상담    받  으러    가    아   보    면  ...                   \n",
      "1        우리  지역구   이   ᆫ데  금태섭   뽑   으면    안   되    지  ...                   \n",
      "2        ㅋㅋ   복수   하   ᆯ겨                              ...                   \n",
      "3       당연히   많이   보    ᆯ    수  밖에  ...                ...                   \n",
      "4        호주   에서   는   남자    들   이   동물   보다  못하    ᆫ  ...                   \n",
      "...     ...  ...  ..  ...  ...  ..  ...  ...  ..  ...  ...  ..  ..  ..  ..   \n",
      "21769     '    '  게'    '    '   나    '    '  고동    '  ...                   \n",
      "21770     '    '  쌍욕   쟁이    '   '    '   무상  연애  주의자  ...                   \n",
      "21771  #@이름    #   저  도라이   국민   의    힘    에   ᆫ    왜  ...                   \n",
      "21772     야    는  아직    도  대통령   '    '  병자'   '    '  ...                   \n",
      "21773    내역    을   잘    보  아야지   ,   그거    ᆫ  가끔    이  ...                   \n",
      "\n",
      "      643 644 645 646 647 648  \n",
      "0                           0  \n",
      "1                           0  \n",
      "2                           0  \n",
      "3                           0  \n",
      "4                           0  \n",
      "...    ..  ..  ..  ..  ..  ..  \n",
      "21769                       1  \n",
      "21770                       1  \n",
      "21771                       1  \n",
      "21772                       1  \n",
      "21773                       1  \n",
      "\n",
      "[21774 rows x 649 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# DataFrame 생성\n",
    "test_df = pd.DataFrame(index=range(len(test_data[0])), columns=range(max_tokens + 1))  # Add one more column for 'label'\n",
    "\n",
    "# 각 문장에 대해 반복하면서 DataFrame 생성# 각 문장에서 최대 토큰 수 계산\n",
    "# max_tokens = max(len(sentence) for sentence in test_data[0])\n",
    "for i, sentence in enumerate(test_data[0]):\n",
    "    # DataFrame에 토큰 형태 추가\n",
    "    for j, token in enumerate(sentence):\n",
    "        test_df.loc[i, j] = token.form\n",
    "\n",
    "    # 마지막 열에 'label' 값 추가\n",
    "    test_df.loc[i, max_tokens] = test_data[1].iloc[i]  # Use .iloc[i] to access the value at index i\n",
    "\n",
    "# NaN 값을 빈 문자열로 대체\n",
    "test_df = test_df.fillna('')\n",
    "\n",
    "# 결과적인 DataFrame 출력\n",
    "print(test_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        음 그렇 ᆫ가 ?                                     ...\n",
      "1        강원도  꼴랑 700 억 주 고 ㅋㅋㅋㅋㅋㅋ 차이나 타운 같은 거나 만들 어 주 네...\n",
      "2        ' ' 빨갱이 ' ' ' 들 이제 ᆫ 대놓고 축 하하 나 의 과거 도 그렇 고 미래...\n",
      "3        그런 대사 뭐  지 , ? 소설 같 네                         ...\n",
      "4        ' ' 미치 ᆫ ' ' ' . 그야말로 사람  기계 부품 취급 . ' ' 빨갱좌빠리...\n",
      "                               ...                        \n",
      "84541    키키 같이 입사 도전 ?                                 ...\n",
      "84542    애초 에 유재석  우리나라 개돼지 국민 서 으 로 나 좋아하 ᆯ 수 있  인간상  ...\n",
      "84543    대가리 텅 비 ᆫ 년 들 ..                              ...\n",
      "84544    불 에 타  줄 알 았 음 ... 덥 으면 악몽 꾸 잖아 그렇 게 매일 아침 에 깨...\n",
      "84545    아니요 ... 아무 도 없 네요                             ...\n",
      "Name: merged_sentences, Length: 84546, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\administrator\\AppData\\Local\\Temp\\2\\ipykernel_8092\\1581728357.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['merged_sentences'] = train_df.apply(lambda row: ' '.join(row), axis=1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "\n",
    "stop_words = set(['는', '은', '이', '가', '을', '를', '그리고', '그래서', '또한', '그러하니', '그러나', '그리하여', '그리하여금', '그러한', '그러므로', ''])\n",
    "\n",
    "def process_text(text):\n",
    "    if isinstance(text, str):\n",
    "        words = word_tokenize(text)\n",
    "        filtered_words = [word for word in words if word not in stop_words]\n",
    "        return ' '.join(filtered_words)\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# 각 열에 함수를 적용합니다.\n",
    "for column_index in train_df.columns:\n",
    "    train_df[column_index] = train_df[column_index].apply(process_text)\n",
    "\n",
    "# 처리된 열을 하나의 열로 합칩니다.\n",
    "train_df['merged_sentences'] = train_df.apply(lambda row: ' '.join(row), axis=1)\n",
    "\n",
    "print(train_df['merged_sentences'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        은행 에 대출 상담 받 으러  아 보 면 직업 의 귀천 바로 알리 어 주 ᆷ    ...\n",
      "1        우리 지역구  ᆫ데 금태섭 뽑 으면 안 되 지 ? 그래도 자 한 당 뻡아야  겠 지...\n",
      "2        ㅋㅋ 복수 하 ᆯ겨                                    ...\n",
      "3        당연히 많이 보 ᆯ 수 밖에 ...                           ...\n",
      "4        호주 에서  남자 들  동물 보다 못하 ᆫ 존재  라던데 ㅋㅋ            ...\n",
      "                               ...                        \n",
      "21769    ' ' 게 ' ' ' 나 ' ' 고동 ' ' ' 이나 대통령  되 겠 다고 하 는데...\n",
      "21770    ' ' 쌍욕 쟁이 ' ' ' 무상 연애 주의자  ' ' 죄명 ' ' ' 이나 전라도...\n",
      "21771    # @ 이름 # 저 도라이 국민 의 힘 에 ᆫ 왜 저런 놈 만 있 을까 ? 나이 값...\n",
      "21772    야  아직 도 대통령 ' ' 병자 ' ' ' 냐 ? 또 철수 없 으 도 대한민국 잘...\n",
      "21773    내역  잘 보 아야지 , 그거 ᆫ 가끔  고 거의 매일 한 명 당 2-3 만 원 씩...\n",
      "Name: merged_sentences, Length: 21774, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\administrator\\AppData\\Local\\Temp\\2\\ipykernel_8092\\606414554.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df['merged_sentences'] = test_df.apply(lambda row: ' '.join(row), axis=1)\n"
     ]
    }
   ],
   "source": [
    "# 각 열에 함수를 적용합니다.\n",
    "for column_index in test_df.columns:\n",
    "    test_df[column_index] = test_df[column_index].apply(process_text)\n",
    "\n",
    "# 처리된 열을 하나의 열로 합칩니다.\n",
    "test_df['merged_sentences'] = test_df.apply(lambda row: ' '.join(row), axis=1)\n",
    "\n",
    "print(test_df['merged_sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = train_df.iloc[649].values\n",
    "test_label = test_df.iloc[649].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['저런', '거', 'ᆯ', '보', '고', '도', '뭐', '', 'ᆫ지', '도', '모르', 'ᆯ', '만큼',\n",
       "       '관심', '도', '없', '고', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '',\n",
       "       '저런 거 ᆯ 보 고 도 뭐  ᆫ지 도 모르 ᆯ 만큼 관심 도 없 고                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        '],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 텍스트를 TF-IDF 벡터로 변환합니다.\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_df['merged_sentences'])\n",
    "X_test = vectorizer.transform(test_df['merged_sentences'])  # transform만 사용\n",
    "\n",
    "# 랜덤 포레스트 모델을 생성하고 학습합니다.\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train, train_label)\n",
    "\n",
    "# 테스트 데이터에 대한 예측을 수행합니다.\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# 정확도를 출력합니다.\n",
    "accuracy = accuracy_score(test_label, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def print_scores( test_label, y_pred, pred_proba=None):\n",
    "    # 필요한 모듈 임포트\n",
    "    from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, \\\n",
    "        roc_auc_score\n",
    "    \n",
    "    # 오차 행렬 계산\n",
    "    cm = confusion_matrix(test_label, y_pred)\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    \n",
    "    # 다양한 성능 지표 출력\n",
    "    print('오차행렬 \\n', cm)\n",
    "    print('Accuracy_score(정확도) :', accuracy_score(test_label, y_pred))\n",
    "    print('Precision(정밀도) : ', precision_score(test_label, y_pred))\n",
    "    print('Recall(재현율) :', recall_score(test_label, y_pred))\n",
    "    \n",
    "    # TNR은 True Negative Rate로, 실제 0인 것 중에서 모델이 정확하게 예측한 비율입니다.\n",
    "    print('TNR(0을 맞춘 비율) :', TN / (TN + FP))\n",
    "    \n",
    "    # F1 score 계산\n",
    "    print('F1 score :', f1_score(test_label, y_pred))\n",
    "    \n",
    "    # 예측 확률이 주어진 경우에만 Roc Auc score 출력\n",
    "    if pred_proba is not None:\n",
    "        print('Roc Auc score :', roc_auc_score(test_label, pred_proba))\n",
    "\n",
    "        # 혼동 행렬 그리기\n",
    "    cm = confusion_matrix(test_label, y_pred)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "                xticklabels=['Predicted 0', 'Predicted 1'],\n",
    "                yticklabels=['Actual 0', 'Actual 1'])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print_scores(test_label,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_predict(sent):\n",
    "    sentence=True\n",
    "    new_data_transformed = vectorizer.transform([sent])\n",
    "    prediction = rf_model.predict(new_data_transformed)\n",
    "    \n",
    "    if prediction == 0:\n",
    "        sentence = True #정상어\n",
    "\n",
    "    elif prediction == 1:\n",
    "        sentence = False #비속어\n",
    "    return sentence, sent\n",
    "\n",
    "sentence_predict(\"씨발\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run C:\\Users\\administrator\\Desktop\\X_filter\\AI_Model\\LSTM.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def badword_find(sent):\n",
    "    result = sent\n",
    "    badword_df = pd.read_excel(r'C:\\Users\\administrator\\Desktop\\X_filter\\AI_Model\\dataset\\word_list (완성).xlsx')\n",
    "    \n",
    "    found_bad_word = False  # 입력 문장에 단어가 발견되었는지를 나타내는 플래그\n",
    "    for idx, row in badword_df.iterrows():\n",
    "        if row[\"WORD\"] in sent:\n",
    "            # 'WORD'가 입력 문장에 포함된 경우\n",
    "            new_word = row[\"대체어\"]\n",
    "            if not pd.isnull(new_word):\n",
    "                result = result.replace(row[\"WORD\"], new_word)\n",
    "                found_bad_word = True\n",
    "            else:\n",
    "                result = result.replace(row[\"WORD\"], \"*\" * len(row[\"WORD\"]))\n",
    "                found_bad_word = True\n",
    "    \n",
    "    if not found_bad_word:\n",
    "        # result = \"@\" * len(input_sentence)\n",
    "        result = \"혐오 표현입니다.\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speak_pre(sent):\n",
    "    sentence = sentence_predict(sent)\n",
    "    if sentence[0] == True:\n",
    "        return sent\n",
    "    elif sentence[0] == False:\n",
    "        return badword_find(sent)\n",
    "    \n",
    "def speak(sent):\n",
    "    speak_pre(sent)\n",
    "    sentence = sentence_predict(sent)\n",
    "    if sentence[0] == True:\n",
    "        return sent\n",
    "    elif sentence[0] == False:\n",
    "        return badword_find(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특수문자 제거\n",
    "def cleanse(text):\n",
    "    pattern = re.compile(r'\\s+')\n",
    "    text = re.sub(pattern, ' ', text)\n",
    "    text = re.sub('[^가-힣ㄱ-ㅎㅏ|a-zA-Z0-9]','', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from soynlp.normalizer import repeat_normalize\n",
    "\n",
    "def clean_and_repeat_normalize(text):\n",
    "    cleansed_text = cleanse(text)                                             # 특수문자 제거\n",
    "    normalized_text = repeat_normalize(cleansed_text, num_repeats=2)          # 중복문자 제거\n",
    "    input_data = re.sub(r'\\d', '', normalized_text)                           #  제거\n",
    "    # normalized_text = spacing(input_data)                                   # 띄어쓰기 보정 \n",
    "\n",
    "    return input_data\n",
    "\n",
    "clean_and_repeat_normalize(\"아버지가방에들어가신다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kss\n",
    "\n",
    "def final_output():\n",
    "    input_text = clean_and_repeat_normalize(input())\n",
    "    sentences = kss.split_sentences(input_text)\n",
    "\n",
    "    sentences_list = []\n",
    "    for sentence in sentences:\n",
    "        speak(sentence)\n",
    "        sentences_list.append(sentence)\n",
    "\n",
    "    long_test = ' '.join(sentences_list)\n",
    "    print(long_test)\n",
    "    return long_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kcelec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
